2025/01/29 15:42:35 maxprocs: Leaving GOMAXPROCS=48: CPU quota undefined
2025-01-29 15:42:35.175064 I | rookcmd: starting Rook v1.16.2 with arguments '/usr/local/bin/rook ceph operator'
2025-01-29 15:42:35.175117 I | rookcmd: flag values: --enable-machine-disruption-budget=false, --help=false, --kubeconfig=, --log-level=INFO
2025-01-29 15:42:35.175132 I | cephcmd: starting Rook-Ceph operator
2025-01-29 15:42:35.623273 I | cephcmd: base ceph version inside the rook operator image is "ceph version 18.2.4 (e7ad5345525c7aa95470c26863873b581076945d) reef (stable)"
2025-01-29 15:42:35.667212 I | op-k8sutil: ROOK_CURRENT_NAMESPACE_ONLY="false" (env var)
2025-01-29 15:42:35.667238 I | operator: watching all namespaces for Ceph CRs
2025-01-29 15:42:35.667943 I | operator: setting up schemes
2025-01-29 15:42:35.712442 I | op-k8sutil: ROOK_OPERATOR_METRICS_BIND_ADDRESS="0" (default)
2025-01-29 15:42:35.712474 I | operator: setting up the controller-runtime manager
2025-01-29 15:42:35.713723 I | ceph-cluster-controller: successfully started
2025-01-29 15:42:35.722112 I | op-k8sutil: ROOK_DISABLE_DEVICE_HOTPLUG="false" (env var)
2025-01-29 15:42:35.722134 I | ceph-cluster-controller: enabling hotplug orchestration
2025-01-29 15:42:35.722189 I | ceph-nodedaemon-controller: successfully started
2025-01-29 15:42:35.722224 I | ceph-block-pool-controller: successfully started
2025-01-29 15:42:35.722275 I | ceph-object-store-user-controller: successfully started
2025-01-29 15:42:35.722308 I | ceph-object-realm-controller: successfully started
2025-01-29 15:42:35.722328 I | ceph-object-zonegroup-controller: successfully started
2025-01-29 15:42:35.722359 I | ceph-object-zone-controller: successfully started
2025-01-29 15:42:35.723080 I | ceph-object-controller: successfully started
2025-01-29 15:42:35.723171 I | ceph-file-controller: successfully started
2025-01-29 15:42:35.723242 I | ceph-nfs-controller: successfully started
2025-01-29 15:42:35.723280 I | ceph-rbd-mirror-controller: successfully started
2025-01-29 15:42:35.723317 I | ceph-client-controller: successfully started
2025-01-29 15:42:35.723347 I | ceph-filesystem-mirror-controller: successfully started
2025-01-29 15:42:35.723392 I | operator: rook-ceph-operator-config-controller successfully started
2025-01-29 15:42:35.723448 I | ceph-csi: rook-ceph-operator-csi-controller successfully started
2025-01-29 15:42:35.723763 I | op-bucket-prov: rook-ceph-operator-bucket-controller successfully started
2025-01-29 15:42:35.723790 I | ceph-bucket-topic: successfully started
2025-01-29 15:42:35.723822 I | ceph-bucket-notification: successfully started
2025-01-29 15:42:35.723844 I | ceph-bucket-notification: successfully started
2025-01-29 15:42:35.723866 I | ceph-fs-subvolumegroup-controller: successfully started
2025-01-29 15:42:35.723977 I | blockpool-rados-namespace-controller: successfully started
2025-01-29 15:42:35.724089 I | ceph-cosi-controller: successfully started
2025-01-29 15:42:35.724127 I | operator: starting the controller-runtime manager
2025-01-29 15:42:36.435222 I | op-k8sutil: ROOK_CEPH_COMMANDS_TIMEOUT_SECONDS="15" (configmap)
2025-01-29 15:42:36.435258 I | op-k8sutil: ROOK_LOG_LEVEL="INFO" (configmap)
2025-01-29 15:42:36.435280 I | op-k8sutil: ROOK_ENABLE_DISCOVERY_DAEMON="false" (configmap)
2025-01-29 15:42:36.440405 I | op-k8sutil: ROOK_CEPH_ALLOW_LOOP_DEVICES="false" (configmap)
2025-01-29 15:42:36.440431 I | op-k8sutil: ROOK_ENFORCE_HOST_NETWORK="false" (default)
2025-01-29 15:42:36.440448 I | op-k8sutil: ROOK_REVISION_HISTORY_LIMIT="" (default)
2025-01-29 15:42:36.440456 I | operator: rook-ceph-operator-config-controller done reconciling
2025-01-29 15:42:36.485029 I | ceph-csi: successfully created csi config map "rook-ceph-csi-config"
2025-01-29 15:42:36.485094 I | op-k8sutil: ROOK_USE_CSI_OPERATOR="false" (default)
2025-01-29 15:42:36.485110 I | op-k8sutil: ROOK_CSI_DISABLE_DRIVER="false" (configmap)
2025-01-29 15:42:36.485152 I | op-k8sutil: removing daemonset csi-rbdplugin if it exists
2025-01-29 15:42:36.516435 I | op-k8sutil: removing deployment csi-rbdplugin-provisioner if it exists
2025-01-29 15:42:36.529815 I | ceph-csi: successfully removed CSI Ceph RBD driver
2025-01-29 15:42:36.529841 I | op-k8sutil: removing daemonset csi-cephfsplugin if it exists
2025-01-29 15:42:36.624264 I | op-k8sutil: removing deployment csi-cephfsplugin-provisioner if it exists
2025-01-29 15:42:36.671117 I | ceph-csi: successfully removed CSI CephFS driver
2025-01-29 15:42:36.671141 I | op-k8sutil: removing daemonset csi-nfsplugin if it exists
2025-01-29 15:42:36.675765 I | op-k8sutil: removing deployment csi-nfsplugin-provisioner if it exists
2025-01-29 15:42:36.690755 I | ceph-csi: successfully removed CSI NFS driver
2025-01-29 15:42:42.481820 E | clusterdisruption-controller: cephcluster "rook-ceph/" seems to be deleted, not requeuing until triggered again
2025-01-29 15:42:42.482803 I | ceph-spec: adding finalizer "cephblockpool.ceph.rook.io" on "ceph-blockpool"
2025-01-29 15:42:42.496089 E | clusterdisruption-controller: cephcluster "rook-ceph/" seems to be deleted, not requeuing until triggered again
2025-01-29 15:42:42.502778 I | ceph-spec: adding finalizer "cephcluster.ceph.rook.io" on "rook-ceph"
2025-01-29 15:42:42.510524 I | clusterdisruption-controller: deleted all legacy node drain canary pods
2025-01-29 15:42:42.533441 I | ceph-cluster-controller: reconciling ceph cluster in namespace "rook-ceph"
2025-01-29 15:42:42.536159 I | ceph-cluster-controller: clusterInfo not yet found, must be a new cluster.
2025-01-29 15:42:42.544806 I | ceph-csi: cluster info for cluster "rook-ceph" is not ready yet, will retry in 10s, proceeding with ready clusters
2025-01-29 15:42:42.544841 I | op-k8sutil: ROOK_CSI_ENABLE_RBD="true" (configmap)
2025-01-29 15:42:42.544873 I | op-k8sutil: ROOK_CSI_ENABLE_CEPHFS="true" (configmap)
2025-01-29 15:42:42.544889 I | op-k8sutil: ROOK_CSI_ENABLE_NFS="false" (configmap)
2025-01-29 15:42:42.544899 I | op-k8sutil: CSI_ENABLE_HOST_NETWORK="true" (configmap)
2025-01-29 15:42:42.544908 I | op-k8sutil: CSI_FORCE_CEPHFS_KERNEL_CLIENT="true" (configmap)
2025-01-29 15:42:42.544918 I | op-k8sutil: CSI_GRPC_TIMEOUT_SECONDS="150" (configmap)
2025-01-29 15:42:42.544932 I | op-k8sutil: CSI_CEPHFS_LIVENESS_METRICS_PORT="9081" (default)
2025-01-29 15:42:42.544943 I | op-k8sutil: CSIADDONS_PORT="9070" (default)
2025-01-29 15:42:42.544953 I | op-k8sutil: CSI_RBD_LIVENESS_METRICS_PORT="9080" (default)
2025-01-29 15:42:42.544964 I | op-k8sutil: CSI_ENABLE_LIVENESS="true" (configmap)
2025-01-29 15:42:42.544996 I | op-k8sutil: CSI_PLUGIN_PRIORITY_CLASSNAME="system-node-critical" (configmap)
2025-01-29 15:42:42.545006 I | op-k8sutil: CSI_PROVISIONER_PRIORITY_CLASSNAME="system-cluster-critical" (configmap)
2025-01-29 15:42:42.545016 I | op-k8sutil: CSI_ENABLE_OMAP_GENERATOR="false" (configmap)
2025-01-29 15:42:42.545026 I | op-k8sutil: CSI_ENABLE_RBD_SNAPSHOTTER="true" (configmap)
2025-01-29 15:42:42.545035 I | op-k8sutil: CSI_ENABLE_CEPHFS_SNAPSHOTTER="true" (configmap)
2025-01-29 15:42:42.545058 I | op-k8sutil: CSI_ENABLE_NFS_SNAPSHOTTER="true" (configmap)
2025-01-29 15:42:42.548123 I | op-k8sutil: CSI_ENABLE_CSIADDONS="false" (configmap)
2025-01-29 15:42:42.548151 I | op-k8sutil: CSI_ENABLE_TOPOLOGY="false" (configmap)
2025-01-29 15:42:42.548168 I | op-k8sutil: CSI_ENABLE_ENCRYPTION="false" (configmap)
2025-01-29 15:42:42.548182 I | op-k8sutil: CSI_ENABLE_METADATA="false" (configmap)
2025-01-29 15:42:42.548200 I | op-k8sutil: CSI_CEPHFS_PLUGIN_UPDATE_STRATEGY="RollingUpdate" (default)
2025-01-29 15:42:42.548217 I | op-k8sutil: CSI_CEPHFS_PLUGIN_UPDATE_STRATEGY_MAX_UNAVAILABLE="1" (default)
2025-01-29 15:42:42.548234 I | op-k8sutil: CSI_NFS_PLUGIN_UPDATE_STRATEGY="RollingUpdate" (default)
2025-01-29 15:42:42.548251 I | op-k8sutil: CSI_RBD_PLUGIN_UPDATE_STRATEGY="RollingUpdate" (default)
2025-01-29 15:42:42.548269 I | op-k8sutil: CSI_RBD_PLUGIN_UPDATE_STRATEGY_MAX_UNAVAILABLE="1" (default)
2025-01-29 15:42:42.548286 I | op-k8sutil: CSI_PLUGIN_ENABLE_SELINUX_HOST_MOUNT="false" (configmap)
2025-01-29 15:42:42.548302 I | op-k8sutil: CSI_LOG_LEVEL="" (default)
2025-01-29 15:42:42.548319 I | op-k8sutil: CSI_SIDECAR_LOG_LEVEL="" (default)
2025-01-29 15:42:42.548334 I | op-k8sutil: CSI_LEADER_ELECTION_LEASE_DURATION="" (default)
2025-01-29 15:42:42.548349 I | op-k8sutil: CSI_LEADER_ELECTION_RENEW_DEADLINE="" (default)
2025-01-29 15:42:42.548365 I | op-k8sutil: CSI_LEADER_ELECTION_RETRY_PERIOD="" (default)
2025-01-29 15:42:42.552803 I | op-k8sutil: CSI_PROVISIONER_REPLICAS="2" (configmap)
2025-01-29 15:42:42.552833 I | op-k8sutil: ROOK_CSI_CEPH_IMAGE="quay.io/cephcsi/cephcsi:v3.13.0" (configmap)
2025-01-29 15:42:42.552854 I | op-k8sutil: ROOK_CSI_REGISTRAR_IMAGE="registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.13.0" (configmap)
2025-01-29 15:42:42.552898 I | op-k8sutil: ROOK_CSI_PROVISIONER_IMAGE="registry.k8s.io/sig-storage/csi-provisioner:v5.1.0" (configmap)
2025-01-29 15:42:42.552920 I | op-k8sutil: ROOK_CSI_ATTACHER_IMAGE="registry.k8s.io/sig-storage/csi-attacher:v4.8.0" (configmap)
2025-01-29 15:42:42.552938 I | op-k8sutil: ROOK_CSI_SNAPSHOTTER_IMAGE="registry.k8s.io/sig-storage/csi-snapshotter:v8.2.0" (configmap)
2025-01-29 15:42:42.552957 I | op-k8sutil: ROOK_CSI_RESIZER_IMAGE="registry.k8s.io/sig-storage/csi-resizer:v1.13.1" (configmap)
2025-01-29 15:42:42.552978 I | op-k8sutil: ROOK_CSI_KUBELET_DIR_PATH="/var/lib/kubelet" (default)
2025-01-29 15:42:42.552997 I | op-k8sutil: ROOK_CSIADDONS_IMAGE="quay.io/csiaddons/k8s-sidecar:v0.11.0" (configmap)
2025-01-29 15:42:42.553014 I | op-k8sutil: CSI_TOPOLOGY_DOMAIN_LABELS="" (default)
2025-01-29 15:42:42.553053 I | op-k8sutil: ROOK_CSI_CEPHFS_POD_LABELS="" (default)
2025-01-29 15:42:42.553079 I | op-k8sutil: ROOK_CSI_NFS_POD_LABELS="" (default)
2025-01-29 15:42:42.553101 I | op-k8sutil: ROOK_CSI_RBD_POD_LABELS="" (default)
2025-01-29 15:42:42.553126 I | op-k8sutil: CSI_CLUSTER_NAME="" (default)
2025-01-29 15:42:42.553147 I | op-k8sutil: ROOK_CSI_IMAGE_PULL_POLICY="IfNotPresent" (configmap)
2025-01-29 15:42:42.553166 I | op-k8sutil: CSI_CEPHFS_KERNEL_MOUNT_OPTIONS="ms_mode=prefer-crc" (configmap)
2025-01-29 15:42:42.553185 I | op-k8sutil: CSI_CEPHFS_ATTACH_REQUIRED="true" (configmap)
2025-01-29 15:42:42.553203 I | op-k8sutil: CSI_RBD_ATTACH_REQUIRED="true" (configmap)
2025-01-29 15:42:42.553219 I | op-k8sutil: CSI_NFS_ATTACH_REQUIRED="true" (configmap)
2025-01-29 15:42:42.553237 I | op-k8sutil: CSI_DRIVER_NAME_PREFIX="rook-ceph" (default)
2025-01-29 15:42:42.557593 I | op-k8sutil: CSI_ENABLE_VOLUME_GROUP_SNAPSHOT="true" (configmap)
2025-01-29 15:42:42.557622 I | op-k8sutil: CSI_KUBE_API_BURST="" (default)
2025-01-29 15:42:42.557634 I | op-k8sutil: CSI_KUBE_API_QPS="" (default)
2025-01-29 15:42:42.559193 I | ceph-spec: detecting the ceph image version for image quay.io/ceph/ceph:v19.2.0...
2025-01-29 15:42:42.582304 I | op-k8sutil: CSI_PROVISIONER_TOLERATIONS="" (default)
2025-01-29 15:42:42.582330 I | op-k8sutil: CSI_PROVISIONER_NODE_AFFINITY="" (default)
2025-01-29 15:42:42.582342 I | op-k8sutil: CSI_PLUGIN_TOLERATIONS="" (default)
2025-01-29 15:42:42.582351 I | op-k8sutil: CSI_PLUGIN_NODE_AFFINITY="" (default)
2025-01-29 15:42:42.582360 I | op-k8sutil: CSI_RBD_PLUGIN_TOLERATIONS="" (default)
2025-01-29 15:42:42.582368 I | op-k8sutil: CSI_RBD_PLUGIN_NODE_AFFINITY="" (default)
2025-01-29 15:42:42.582390 I | op-k8sutil: CSI_RBD_PLUGIN_RESOURCE="- name : driver-registrar\n  resource:\n    requests:\n      memory: 128Mi\n      cpu: 50m\n    limits:\n      memory: 256Mi\n- name : csi-rbdplugin\n  resource:\n    requests:\n      memory: 512Mi\n      cpu: 250m\n    limits:\n      memory: 1Gi\n- name : liveness-prometheus\n  resource:\n    requests:\n      memory: 128Mi\n      cpu: 50m\n    limits:\n      memory: 256Mi\n" (configmap)
2025-01-29 15:42:42.582691 I | op-k8sutil: CSI_RBD_PLUGIN_VOLUME="" (default)
2025-01-29 15:42:42.582707 I | op-k8sutil: CSI_RBD_PLUGIN_VOLUME_MOUNT="" (default)
2025-01-29 15:42:42.597503 I | op-k8sutil: CSI_RBD_PROVISIONER_TOLERATIONS="" (default)
2025-01-29 15:42:42.597530 I | op-k8sutil: CSI_RBD_PROVISIONER_NODE_AFFINITY="" (default)
2025-01-29 15:42:42.597564 I | op-k8sutil: CSI_RBD_PROVISIONER_RESOURCE="- name : csi-provisioner\n  resource:\n    requests:\n      memory: 128Mi\n      cpu: 100m\n    limits:\n      memory: 256Mi\n- name : csi-resizer\n  resource:\n    requests:\n      memory: 128Mi\n      cpu: 100m\n    limits:\n      memory: 256Mi\n- name : csi-attacher\n  resource:\n    requests:\n      memory: 128Mi\n      cpu: 100m\n    limits:\n      memory: 256Mi\n- name : csi-snapshotter\n  resource:\n    requests:\n      memory: 128Mi\n      cpu: 100m\n    limits:\n      memory: 256Mi\n- name : csi-rbdplugin\n  resource:\n    requests:\n      memory: 512Mi\n    limits:\n      memory: 1Gi\n- name : csi-omap-generator\n  resource:\n    requests:\n      memory: 512Mi\n      cpu: 250m\n    limits:\n      memory: 1Gi\n- name : liveness-prometheus\n  resource:\n    requests:\n      memory: 128Mi\n      cpu: 50m\n    limits:\n      memory: 256Mi\n" (configmap)
2025-01-29 15:42:42.629879 I | ceph-csi: successfully started CSI Ceph RBD driver
2025-01-29 15:42:42.915606 I | op-k8sutil: CSI_CEPHFS_PLUGIN_TOLERATIONS="" (default)
2025-01-29 15:42:42.915640 I | op-k8sutil: CSI_CEPHFS_PLUGIN_NODE_AFFINITY="" (default)
2025-01-29 15:42:42.915675 I | op-k8sutil: CSI_CEPHFS_PLUGIN_RESOURCE="- name : driver-registrar\n  resource:\n    requests:\n      memory: 128Mi\n      cpu: 50m\n    limits:\n      memory: 256Mi\n- name : csi-cephfsplugin\n  resource:\n    requests:\n      memory: 512Mi\n      cpu: 250m\n    limits:\n      memory: 1Gi\n- name : liveness-prometheus\n  resource:\n    requests:\n      memory: 128Mi\n      cpu: 50m\n    limits:\n      memory: 256Mi\n" (configmap)
2025-01-29 15:42:42.916061 I | op-k8sutil: CSI_CEPHFS_PLUGIN_VOLUME="" (default)
2025-01-29 15:42:42.916079 I | op-k8sutil: CSI_CEPHFS_PLUGIN_VOLUME_MOUNT="" (default)
2025-01-29 15:42:42.935817 I | op-k8sutil: CSI_CEPHFS_PROVISIONER_TOLERATIONS="" (default)
2025-01-29 15:42:42.935843 I | op-k8sutil: CSI_CEPHFS_PROVISIONER_NODE_AFFINITY="" (default)
2025-01-29 15:42:42.935879 I | op-k8sutil: CSI_CEPHFS_PROVISIONER_RESOURCE="- name : csi-provisioner\n  resource:\n    requests:\n      memory: 128Mi\n      cpu: 100m\n    limits:\n      memory: 256Mi\n- name : csi-resizer\n  resource:\n    requests:\n      memory: 128Mi\n      cpu: 100m\n    limits:\n      memory: 256Mi\n- name : csi-attacher\n  resource:\n    requests:\n      memory: 128Mi\n      cpu: 100m\n    limits:\n      memory: 256Mi\n- name : csi-snapshotter\n  resource:\n    requests:\n      memory: 128Mi\n      cpu: 100m\n    limits:\n      memory: 256Mi\n- name : csi-cephfsplugin\n  resource:\n    requests:\n      memory: 512Mi\n      cpu: 250m\n    limits:\n      memory: 1Gi\n- name : liveness-prometheus\n  resource:\n    requests:\n      memory: 128Mi\n      cpu: 50m\n    limits:\n      memory: 256Mi\n" (configmap)
2025-01-29 15:42:42.954064 I | ceph-csi: successfully started CSI CephFS driver
2025-01-29 15:42:43.315751 I | op-k8sutil: CSI_RBD_FSGROUPPOLICY="File" (configmap)
2025-01-29 15:42:43.325590 I | ceph-csi: CSIDriver object created for driver "rook-ceph.rbd.csi.ceph.com"
2025-01-29 15:42:43.325615 I | op-k8sutil: CSI_CEPHFS_FSGROUPPOLICY="File" (configmap)
2025-01-29 15:42:43.333282 I | ceph-csi: CSIDriver object created for driver "rook-ceph.cephfs.csi.ceph.com"
2025-01-29 15:42:43.333319 I | op-k8sutil: removing daemonset csi-nfsplugin if it exists
2025-01-29 15:42:43.336342 I | op-k8sutil: removing deployment csi-nfsplugin-provisioner if it exists
2025-01-29 15:42:43.528453 I | ceph-csi: successfully removed CSI NFS driver
2025-01-29 15:42:46.153243 I | ceph-spec: detected ceph image version: "19.2.0-0 squid"
2025-01-29 15:42:46.153273 I | ceph-cluster-controller: validating ceph version from provided image
2025-01-29 15:42:46.157684 I | ceph-cluster-controller: cluster "rook-ceph": version "19.2.0-0 squid" detected for image "quay.io/ceph/ceph:v19.2.0"
2025-01-29 15:42:46.208106 E | ceph-spec: failed to update cluster condition to {Type:Progressing Status:True Reason:ClusterProgressing Message:Configuring the Ceph cluster LastHeartbeatTime:2025-01-29 15:42:46.192905904 +0000 UTC m=+11.236252295 LastTransitionTime:2025-01-29 15:42:46.192905724 +0000 UTC m=+11.236252125}. failed to update object "rook-ceph/rook-ceph" status: Operation cannot be fulfilled on cephclusters.ceph.rook.io "rook-ceph": the object has been modified; please apply your changes to the latest version and try again
2025-01-29 15:42:46.237465 I | op-mon: start running mons
2025-01-29 15:42:46.331138 I | ceph-spec: creating mon secrets for a new cluster
2025-01-29 15:42:46.357145 I | op-mon: existing maxMonID not found or failed to load. configmaps "rook-ceph-mon-endpoints" not found
2025-01-29 15:42:46.363878 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":[],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","radosNamespace":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":"","mirrorDaemonCount":0},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data: mapping:{"node":{}} maxMonId:-1 outOfQuorum:]
2025-01-29 15:42:47.133023 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:42:47.133255 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:42:48.762226 I | op-mon: targeting the mon count 3
2025-01-29 15:42:48.809139 I | op-mon: created canary deployment rook-ceph-mon-a-canary
2025-01-29 15:42:48.848556 I | op-mon: created canary deployment rook-ceph-mon-b-canary
2025-01-29 15:42:48.966416 I | op-mon: created canary deployment rook-ceph-mon-c-canary
2025-01-29 15:42:49.531429 I | op-mon: canary monitor deployment rook-ceph-mon-b-canary scheduled to kubernetes-1
2025-01-29 15:42:49.531469 I | op-mon: mon b assigned to node kubernetes-1
2025-01-29 15:42:49.733108 I | op-mon: canary monitor deployment rook-ceph-mon-c-canary scheduled to kubernetes-0
2025-01-29 15:42:49.733134 I | op-mon: mon c assigned to node kubernetes-0
2025-01-29 15:42:52.598940 I | ceph-spec: parsing mon endpoints: 
2025-01-29 15:42:52.598970 W | ceph-spec: ignoring invalid monitor 
2025-01-29 15:42:52.599035 I | op-k8sutil: ROOK_OBC_WATCH_OPERATOR_NAMESPACE="true" (configmap)
2025-01-29 15:42:52.599050 I | op-k8sutil: ROOK_OBC_PROVISIONER_NAME_PREFIX="" (default)
2025-01-29 15:42:52.599061 I | op-bucket-prov: ceph bucket provisioner launched watching for provisioner "rook-ceph.ceph.rook.io/bucket"
2025-01-29 15:42:52.599898 I | op-bucket-prov: successfully reconciled bucket provisioner
I0129 15:42:52.600037       1 manager.go:135] "msg"="starting provisioner" "logger"="objectbucket.io/provisioner-manager" "name"="rook-ceph.ceph.rook.io/bucket"
2025-01-29 15:42:53.575412 I | ceph-spec: parsing mon endpoints: 
2025-01-29 15:42:53.575445 W | ceph-spec: ignoring invalid monitor 
2025-01-29 15:42:53.705768 I | ceph-csi: successfully started CSI Ceph RBD driver
2025-01-29 15:42:53.822395 I | ceph-csi: successfully started CSI CephFS driver
2025-01-29 15:42:54.546954 I | ceph-csi: CSIDriver object updated for driver "rook-ceph.rbd.csi.ceph.com"
2025-01-29 15:42:54.556397 I | ceph-csi: CSIDriver object updated for driver "rook-ceph.cephfs.csi.ceph.com"
2025-01-29 15:42:54.556426 I | op-k8sutil: removing daemonset csi-nfsplugin if it exists
2025-01-29 15:42:54.560907 I | op-k8sutil: removing deployment csi-nfsplugin-provisioner if it exists
2025-01-29 15:42:54.736109 I | op-mon: canary monitor deployment rook-ceph-mon-a-canary scheduled to kubernetes-2
2025-01-29 15:42:54.736134 I | op-mon: mon a assigned to node kubernetes-2
2025-01-29 15:42:54.744696 I | op-mon: cleaning up canary monitor deployment "rook-ceph-mon-a-canary"
2025-01-29 15:42:54.755126 I | op-mon: cleaning up canary monitor deployment "rook-ceph-mon-b-canary"
2025-01-29 15:42:54.765285 I | op-mon: cleaning up canary monitor deployment "rook-ceph-mon-c-canary"
2025-01-29 15:42:54.777443 I | op-mon: creating mon a
2025-01-29 15:42:54.777470 I | op-mon: setting mon "a" endpoints for hostnetwork mode
2025-01-29 15:42:54.937035 I | ceph-csi: successfully removed CSI NFS driver
2025-01-29 15:42:55.536839 I | op-mon: monitor endpoints changed, updating the bootstrap peer token
2025-01-29 15:42:55.536913 I | op-mon: monitor endpoints changed, updating the bootstrap peer token
2025-01-29 15:42:55.537331 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.100.50.102:3300"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","radosNamespace":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":"","mirrorDaemonCount":0},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.100.50.102:3300 mapping:{"node":{"a":{"Name":"kubernetes-2","Hostname":"kubernetes-2","Address":"10.100.50.102"},"b":{"Name":"kubernetes-1","Hostname":"kubernetes-1","Address":"10.100.50.101"},"c":{"Name":"kubernetes-0","Hostname":"kubernetes-0","Address":"10.100.50.100"}}} maxMonId:-1 outOfQuorum:]
2025-01-29 15:42:56.137265 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:42:56.137533 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:42:56.543789 I | op-mon: 0 of 1 expected mon deployments exist. creating new deployment(s).
2025-01-29 15:42:56.734478 I | op-mon: updating maxMonID from -1 to 0
2025-01-29 15:42:57.536197 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.100.50.102:3300"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","radosNamespace":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":"","mirrorDaemonCount":0},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.100.50.102:3300 mapping:{"node":{"a":{"Name":"kubernetes-2","Hostname":"kubernetes-2","Address":"10.100.50.102"},"b":{"Name":"kubernetes-1","Hostname":"kubernetes-1","Address":"10.100.50.101"},"c":{"Name":"kubernetes-0","Hostname":"kubernetes-0","Address":"10.100.50.100"}}} maxMonId:0 outOfQuorum:]
2025-01-29 15:42:57.536234 I | op-mon: waiting for mon quorum with [a]
2025-01-29 15:42:57.741539 I | op-mon: mon a is not yet running
2025-01-29 15:42:57.741574 I | op-mon: mons running: []
2025-01-29 15:42:59.444060 I | op-mon: Monitors in quorum: [a]
2025-01-29 15:42:59.444093 I | op-mon: mons created: 1
2025-01-29 15:42:59.693577 I | op-mon: waiting for mon quorum with [a]
2025-01-29 15:42:59.705095 I | op-mon: mon a is not yet running
2025-01-29 15:42:59.705119 I | op-mon: mons running: []
2025-01-29 15:43:03.065576 I | clusterdisruption-controller: all PGs are active+clean. Restoring default OSD pdb settings
2025-01-29 15:43:03.065603 I | clusterdisruption-controller: creating the default pdb "rook-ceph-osd" with maxUnavailable=1 for all osd
2025-01-29 15:43:03.078356 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2025-01-29 15:43:03.440963 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2025-01-29 15:43:03.708416 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2025-01-29 15:43:04.716890 I | op-mon: mons running: [a]
2025-01-29 15:43:04.979885 I | op-mon: Monitors in quorum: [a]
2025-01-29 15:43:04.980283 I | op-config: applying ceph settings:
[global]
mon allow pool delete   = true
mon cluster log file    = 
mon allow pool size one = true
2025-01-29 15:43:05.263274 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:43:05.263585 I | op-config: applying ceph settings:
[global]
log to file = true
2025-01-29 15:43:05.524375 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:43:05.524473 I | op-config: deleting "global" "log file" option from the mon configuration database
2025-01-29 15:43:05.829724 I | op-config: successfully deleted "log file" option from the mon configuration database
2025-01-29 15:43:05.829763 I | op-mon: creating mon b
2025-01-29 15:43:05.829774 I | op-mon: setting mon "a" endpoints for hostnetwork mode
2025-01-29 15:43:05.829794 I | op-mon: setting mon "b" endpoints for hostnetwork mode
2025-01-29 15:43:05.858838 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.100.50.102:3300","10.100.50.101:3300"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","radosNamespace":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":"","mirrorDaemonCount":0},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.100.50.102:3300,b=10.100.50.101:3300 mapping:{"node":{"a":{"Name":"kubernetes-2","Hostname":"kubernetes-2","Address":"10.100.50.102"},"b":{"Name":"kubernetes-1","Hostname":"kubernetes-1","Address":"10.100.50.101"},"c":{"Name":"kubernetes-0","Hostname":"kubernetes-0","Address":"10.100.50.100"}}} maxMonId:0 outOfQuorum:]
2025-01-29 15:43:05.875239 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:43:05.875560 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:43:05.896176 I | op-mon: 1 of 2 expected mon deployments exist. creating new deployment(s).
2025-01-29 15:43:05.905746 I | op-mon: deployment for mon rook-ceph-mon-a already exists. updating if needed
2025-01-29 15:43:05.920801 I | op-k8sutil: deployment "rook-ceph-mon-a" did not change, nothing to update
2025-01-29 15:43:05.947115 I | op-mon: updating maxMonID from 0 to 1
2025-01-29 15:43:06.436703 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.100.50.101:3300","10.100.50.102:3300"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","radosNamespace":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":"","mirrorDaemonCount":0},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.100.50.102:3300,b=10.100.50.101:3300 mapping:{"node":{"a":{"Name":"kubernetes-2","Hostname":"kubernetes-2","Address":"10.100.50.102"},"b":{"Name":"kubernetes-1","Hostname":"kubernetes-1","Address":"10.100.50.101"},"c":{"Name":"kubernetes-0","Hostname":"kubernetes-0","Address":"10.100.50.100"}}} maxMonId:1 outOfQuorum:]
2025-01-29 15:43:06.436743 I | op-mon: waiting for mon quorum with [a b]
2025-01-29 15:43:06.842068 I | op-mon: mon b is not yet running
2025-01-29 15:43:06.842096 I | op-mon: mons running: [a]
2025-01-29 15:43:07.096891 I | op-mon: Monitors in quorum: [a]
2025-01-29 15:43:07.096922 I | op-mon: mons created: 2
2025-01-29 15:43:07.372893 I | op-mon: waiting for mon quorum with [a b]
2025-01-29 15:43:07.396234 I | op-mon: mon b is not yet running
2025-01-29 15:43:07.396259 I | op-mon: mons running: [a]
2025-01-29 15:43:12.421875 I | op-mon: mons running: [a b]
2025-01-29 15:43:15.685287 I | op-mon: Monitors in quorum: [a b]
2025-01-29 15:43:15.685700 I | op-config: applying ceph settings:
[global]
mon allow pool delete   = true
mon cluster log file    = 
mon allow pool size one = true
2025-01-29 15:43:15.941648 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:43:15.941919 I | op-config: applying ceph settings:
[global]
log to file = true
2025-01-29 15:43:16.163807 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:43:16.163911 I | op-config: deleting "global" "log file" option from the mon configuration database
2025-01-29 15:43:16.405843 I | op-config: successfully deleted "log file" option from the mon configuration database
2025-01-29 15:43:16.405874 I | op-mon: creating mon c
2025-01-29 15:43:16.405881 I | op-mon: setting mon "a" endpoints for hostnetwork mode
2025-01-29 15:43:16.405891 I | op-mon: setting mon "b" endpoints for hostnetwork mode
2025-01-29 15:43:16.405897 I | op-mon: setting mon "c" endpoints for hostnetwork mode
2025-01-29 15:43:16.428198 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.100.50.100:3300","10.100.50.102:3300","10.100.50.101:3300"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","radosNamespace":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":"","mirrorDaemonCount":0},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300 mapping:{"node":{"a":{"Name":"kubernetes-2","Hostname":"kubernetes-2","Address":"10.100.50.102"},"b":{"Name":"kubernetes-1","Hostname":"kubernetes-1","Address":"10.100.50.101"},"c":{"Name":"kubernetes-0","Hostname":"kubernetes-0","Address":"10.100.50.100"}}} maxMonId:1 outOfQuorum:]
2025-01-29 15:43:16.443939 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:43:16.444195 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:43:16.463005 I | op-mon: 2 of 3 expected mon deployments exist. creating new deployment(s).
2025-01-29 15:43:16.470810 I | op-mon: deployment for mon rook-ceph-mon-a already exists. updating if needed
2025-01-29 15:43:16.489163 I | op-k8sutil: deployment "rook-ceph-mon-a" did not change, nothing to update
2025-01-29 15:43:16.495129 I | op-mon: deployment for mon rook-ceph-mon-b already exists. updating if needed
2025-01-29 15:43:16.509493 I | op-k8sutil: deployment "rook-ceph-mon-b" did not change, nothing to update
2025-01-29 15:43:16.533959 I | op-mon: updating maxMonID from 1 to 2
2025-01-29 15:43:17.011713 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.100.50.102:3300","10.100.50.101:3300","10.100.50.100:3300"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","radosNamespace":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":"","mirrorDaemonCount":0},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300 mapping:{"node":{"a":{"Name":"kubernetes-2","Hostname":"kubernetes-2","Address":"10.100.50.102"},"b":{"Name":"kubernetes-1","Hostname":"kubernetes-1","Address":"10.100.50.101"},"c":{"Name":"kubernetes-0","Hostname":"kubernetes-0","Address":"10.100.50.100"}}} maxMonId:2 outOfQuorum:]
2025-01-29 15:43:17.011745 I | op-mon: waiting for mon quorum with [a b c]
2025-01-29 15:43:17.618260 I | op-mon: mon c is not yet running
2025-01-29 15:43:17.618302 I | op-mon: mons running: [a b]
2025-01-29 15:43:17.945783 I | op-mon: Monitors in quorum: [a b]
2025-01-29 15:43:17.945810 I | op-mon: mons created: 3
2025-01-29 15:43:18.213015 I | op-mon: waiting for mon quorum with [a b c]
2025-01-29 15:43:18.252430 I | op-mon: mon c is not yet running
2025-01-29 15:43:18.252457 I | op-mon: mons running: [a b]
2025-01-29 15:43:23.288922 I | op-mon: mons running: [a b c]
2025-01-29 15:43:24.947674 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:43:24.948067 I | op-config: applying ceph settings:
[global]
mon allow pool delete   = true
mon cluster log file    = 
mon allow pool size one = true
2025-01-29 15:43:25.208107 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:43:25.208446 I | op-config: applying ceph settings:
[global]
log to file = true
2025-01-29 15:43:25.453720 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:43:25.453802 I | op-config: deleting "global" "log file" option from the mon configuration database
2025-01-29 15:43:25.734038 I | op-config: successfully deleted "log file" option from the mon configuration database
2025-01-29 15:43:25.734078 I | ceph-spec: ensuring cluster "rook-ceph" "public" network is configured to use CIDR(s) ""
2025-01-29 15:43:25.734086 I | ceph-spec: ensuring cluster "rook-ceph" "cluster" network is configured to use CIDR(s) ""
2025-01-29 15:43:25.734333 I | cephclient: getting or creating ceph auth key "client.csi-rbd-provisioner"
2025-01-29 15:43:26.023323 I | cephclient: getting or creating ceph auth key "client.csi-rbd-node"
2025-01-29 15:43:26.322581 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-provisioner"
2025-01-29 15:43:26.665455 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-node"
2025-01-29 15:43:26.991664 I | ceph-csi: created kubernetes csi secrets for cluster "rook-ceph"
2025-01-29 15:43:26.991698 I | cephclient: getting or creating ceph auth key "client.crash"
2025-01-29 15:43:27.322865 I | ceph-nodedaemon-controller: created kubernetes crash collector secret for cluster "rook-ceph"
2025-01-29 15:43:27.322899 I | cephclient: getting or creating ceph auth key "client.ceph-exporter"
2025-01-29 15:43:27.620538 I | ceph-nodedaemon-controller: created kubernetes exporter secret for cluster "rook-ceph"
2025-01-29 15:43:27.620574 I | op-config: deleting "global" "ms_client_mode" option from the mon configuration database
2025-01-29 15:43:27.900995 I | op-config: successfully deleted "ms_client_mode" option from the mon configuration database
2025-01-29 15:43:27.901026 I | op-config: deleting "global" "rbd_default_map_options" option from the mon configuration database
2025-01-29 15:43:28.161376 I | op-config: successfully deleted "rbd_default_map_options" option from the mon configuration database
2025-01-29 15:43:28.161410 I | op-config: deleting "global" "ms_cluster_mode" option from the mon configuration database
2025-01-29 15:43:28.424022 I | op-config: successfully deleted "ms_cluster_mode" option from the mon configuration database
2025-01-29 15:43:28.424059 I | op-config: deleting "global" "ms_service_mode" option from the mon configuration database
2025-01-29 15:43:28.674356 I | op-config: successfully deleted "ms_service_mode" option from the mon configuration database
2025-01-29 15:43:28.674681 I | op-config: applying ceph settings:
[global]
rbd_default_map_options = ms_mode=prefer-crc
2025-01-29 15:43:28.932200 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:43:28.932280 I | op-config: deleting "global" "ms_osd_compress_mode" option from the mon configuration database
2025-01-29 15:43:29.175622 I | op-config: successfully deleted "ms_osd_compress_mode" option from the mon configuration database
2025-01-29 15:43:29.175652 I | cephclient: create rbd-mirror bootstrap peer token "client.rbd-mirror-peer"
2025-01-29 15:43:29.175661 I | cephclient: getting or creating ceph auth key "client.rbd-mirror-peer"
2025-01-29 15:43:29.437428 I | cephclient: successfully created rbd-mirror bootstrap peer token for cluster "rook-ceph"
2025-01-29 15:43:29.470545 I | op-mgr: start running mgr
2025-01-29 15:43:29.478329 I | cephclient: getting or creating ceph auth key "mgr.a"
2025-01-29 15:43:29.792798 I | cephclient: getting or creating ceph auth key "mgr.b"
2025-01-29 15:43:29.883491 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:29.993559 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:30.060943 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:30.584990 I | op-config: setting option "auth_allow_insecure_global_id_reclaim" (user "mon") to the mon configuration database
2025-01-29 15:43:30.889527 I | op-config: successfully set option "auth_allow_insecure_global_id_reclaim" (user "mon") to the mon configuration database
2025-01-29 15:43:30.889555 I | op-config: insecure global ID is now disabled
2025-01-29 15:43:30.969949 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:31.571002 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:32.170403 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:32.770065 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:33.359964 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2025-01-29 15:43:33.369558 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:33.704486 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2025-01-29 15:43:33.969681 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:34.570144 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:35.170678 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:35.772503 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:36.370042 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:36.971613 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:37.570208 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:38.169418 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:38.770508 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:39.369909 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:39.970220 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:40.570422 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:41.920362 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:42.602585 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:42.923355 I | op-k8sutil: finished waiting for updated deployment "rook-ceph-mgr-a"
2025-01-29 15:43:44.553091 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:46.648449 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:46.733386 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:46.797472 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:47.013721 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:47.615208 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:48.211875 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:48.815370 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:49.416242 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:50.014193 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:50.614789 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:51.214390 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:51.814704 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:51.948673 I | op-k8sutil: finished waiting for updated deployment "rook-ceph-mgr-b"
2025-01-29 15:43:53.040968 E | ceph-cluster-controller: failed to reconcile CephCluster "rook-ceph/rook-ceph". failed to reconcile cluster "rook-ceph": failed to configure local ceph cluster: failed to create cluster: failed to start ceph mgr: failed to enable mgr services: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-mgr" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:53.046562 I | ceph-cluster-controller: reconciling ceph cluster in namespace "rook-ceph"
2025-01-29 15:43:53.411217 I | ceph-spec: parsing mon endpoints: a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300
2025-01-29 15:43:53.423834 I | ceph-cluster-controller: enabling ceph mon monitoring goroutine for cluster "rook-ceph"
2025-01-29 15:43:53.423894 I | op-osd: ceph osd status in namespace "rook-ceph" check interval "1m0s"
2025-01-29 15:43:53.423904 I | ceph-cluster-controller: enabling ceph osd monitoring goroutine for cluster "rook-ceph"
2025-01-29 15:43:53.423918 I | ceph-cluster-controller: ceph status check interval is 1m0s
2025-01-29 15:43:53.423927 I | ceph-cluster-controller: enabling ceph status monitoring goroutine for cluster "rook-ceph"
2025-01-29 15:43:53.637753 I | ceph-spec: detecting the ceph image version for image quay.io/ceph/ceph:v19.2.0...
2025-01-29 15:43:54.614645 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:43:56.415124 I | ceph-spec: detected ceph image version: "19.2.0-0 squid"
2025-01-29 15:43:56.415169 I | ceph-cluster-controller: validating ceph version from provided image
2025-01-29 15:43:56.423799 I | ceph-spec: parsing mon endpoints: a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300
2025-01-29 15:43:56.428375 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:43:56.428623 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:43:57.133231 I | ceph-cluster-controller: cluster "rook-ceph": version "19.2.0-0 squid" detected for image "quay.io/ceph/ceph:v19.2.0"
2025-01-29 15:43:57.234004 I | op-mon: start running mons
2025-01-29 15:43:57.253868 I | ceph-spec: parsing mon endpoints: a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300
2025-01-29 15:43:57.303955 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.100.50.100:3300","10.100.50.102:3300","10.100.50.101:3300"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","radosNamespace":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":"","mirrorDaemonCount":0},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300 mapping:{"node":{"a":{"Name":"kubernetes-2","Hostname":"kubernetes-2","Address":"10.100.50.102"},"b":{"Name":"kubernetes-1","Hostname":"kubernetes-1","Address":"10.100.50.101"},"c":{"Name":"kubernetes-0","Hostname":"kubernetes-0","Address":"10.100.50.100"}}} maxMonId:2 outOfQuorum:]
2025-01-29 15:43:57.810856 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:43:57.811130 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:43:59.611438 I | op-mon: targeting the mon count 3
2025-01-29 15:43:59.632915 I | op-config: applying ceph settings:
[global]
mon allow pool delete   = true
mon cluster log file    = 
mon allow pool size one = true
2025-01-29 15:44:00.014773 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:44:00.146514 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:44:00.146878 I | op-config: applying ceph settings:
[global]
log to file = true
2025-01-29 15:44:00.615140 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:44:00.675772 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:44:00.675839 I | op-config: deleting "global" "log file" option from the mon configuration database
2025-01-29 15:44:01.076678 I | op-config: successfully deleted "log file" option from the mon configuration database
2025-01-29 15:44:01.076701 I | op-mon: checking for basic quorum with existing mons
2025-01-29 15:44:01.076709 I | op-mon: setting mon "a" endpoints for hostnetwork mode
2025-01-29 15:44:01.076719 I | op-mon: setting mon "b" endpoints for hostnetwork mode
2025-01-29 15:44:01.076725 I | op-mon: setting mon "c" endpoints for hostnetwork mode
2025-01-29 15:44:01.214732 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:44:01.814087 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.100.50.102:3300","10.100.50.101:3300","10.100.50.100:3300"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","radosNamespace":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":"","mirrorDaemonCount":0},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300 mapping:{"node":{"a":{"Name":"kubernetes-2","Hostname":"kubernetes-2","Address":"10.100.50.102"},"b":{"Name":"kubernetes-1","Hostname":"kubernetes-1","Address":"10.100.50.101"},"c":{"Name":"kubernetes-0","Hostname":"kubernetes-0","Address":"10.100.50.100"}}} maxMonId:2 outOfQuorum:]
2025-01-29 15:44:02.409881 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:44:02.410132 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:44:03.032689 I | op-mon: deployment for mon rook-ceph-mon-a already exists. updating if needed
2025-01-29 15:44:03.047853 I | op-k8sutil: deployment "rook-ceph-mon-a" did not change, nothing to update
2025-01-29 15:44:03.047887 I | op-mon: waiting for mon quorum with [a b c]
2025-01-29 15:44:03.209797 I | ceph-spec: parsing mon endpoints: a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300
2025-01-29 15:44:03.642231 I | ceph-block-pool-controller: creating pool "ceph-blockpool" in namespace "rook-ceph"
2025-01-29 15:44:03.820586 I | op-mon: mons running: [a b c]
2025-01-29 15:44:03.956022 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2025-01-29 15:44:04.380073 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:44:04.388522 I | op-mon: deployment for mon rook-ceph-mon-b already exists. updating if needed
2025-01-29 15:44:04.403000 I | op-k8sutil: deployment "rook-ceph-mon-b" did not change, nothing to update
2025-01-29 15:44:04.403051 I | op-mon: waiting for mon quorum with [a b c]
2025-01-29 15:44:04.448924 I | op-mon: mons running: [a b c]
2025-01-29 15:44:04.756408 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2025-01-29 15:44:06.170659 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:44:06.178922 I | op-mon: deployment for mon rook-ceph-mon-c already exists. updating if needed
2025-01-29 15:44:06.193688 I | op-k8sutil: deployment "rook-ceph-mon-c" did not change, nothing to update
2025-01-29 15:44:06.193720 I | op-mon: waiting for mon quorum with [a b c]
2025-01-29 15:44:06.239489 I | op-mon: mons running: [a b c]
2025-01-29 15:44:06.770049 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:44:06.770076 I | op-mon: mons created: 3
2025-01-29 15:44:07.236445 I | cephclient: reconciling replicated pool ceph-blockpool succeeded
2025-01-29 15:44:07.236478 I | ceph-block-pool-controller: initializing pool "ceph-blockpool" for RBD use
2025-01-29 15:44:07.328067 I | op-mon: waiting for mon quorum with [a b c]
2025-01-29 15:44:07.375263 I | op-mon: mons running: [a b c]
2025-01-29 15:44:07.923302 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:44:07.923339 I | ceph-spec: ensuring cluster "rook-ceph" "public" network is configured to use CIDR(s) ""
2025-01-29 15:44:07.923346 I | ceph-spec: ensuring cluster "rook-ceph" "cluster" network is configured to use CIDR(s) ""
2025-01-29 15:44:07.923563 I | cephclient: getting or creating ceph auth key "client.csi-rbd-provisioner"
2025-01-29 15:44:08.613523 I | cephclient: getting or creating ceph auth key "client.csi-rbd-node"
2025-01-29 15:44:09.174401 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-provisioner"
2025-01-29 15:44:09.687535 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-node"
2025-01-29 15:44:10.335665 I | ceph-csi: created kubernetes csi secrets for cluster "rook-ceph"
2025-01-29 15:44:10.335700 I | cephclient: getting or creating ceph auth key "client.crash"
2025-01-29 15:44:10.899645 I | ceph-nodedaemon-controller: created kubernetes crash collector secret for cluster "rook-ceph"
2025-01-29 15:44:10.899685 I | cephclient: getting or creating ceph auth key "client.ceph-exporter"
2025-01-29 15:44:10.930198 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:44:11.458864 I | ceph-nodedaemon-controller: created kubernetes exporter secret for cluster "rook-ceph"
2025-01-29 15:44:11.458902 I | op-config: deleting "global" "ms_client_mode" option from the mon configuration database
2025-01-29 15:44:12.004522 I | op-config: successfully deleted "ms_client_mode" option from the mon configuration database
2025-01-29 15:44:12.004550 I | op-config: deleting "global" "rbd_default_map_options" option from the mon configuration database
2025-01-29 15:44:12.723107 I | op-config: successfully deleted "rbd_default_map_options" option from the mon configuration database
2025-01-29 15:44:12.723147 I | op-config: deleting "global" "ms_cluster_mode" option from the mon configuration database
2025-01-29 15:44:13.159613 I | op-config: successfully deleted "ms_cluster_mode" option from the mon configuration database
2025-01-29 15:44:13.159637 I | op-config: deleting "global" "ms_service_mode" option from the mon configuration database
2025-01-29 15:44:13.643187 I | op-config: successfully deleted "ms_service_mode" option from the mon configuration database
2025-01-29 15:44:13.643547 I | op-config: applying ceph settings:
[global]
rbd_default_map_options = ms_mode=prefer-crc
2025-01-29 15:44:14.128908 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:44:14.129008 I | op-config: deleting "global" "ms_osd_compress_mode" option from the mon configuration database
2025-01-29 15:44:14.555481 I | op-config: successfully deleted "ms_osd_compress_mode" option from the mon configuration database
2025-01-29 15:44:14.555517 I | cephclient: create rbd-mirror bootstrap peer token "client.rbd-mirror-peer"
2025-01-29 15:44:14.555524 I | cephclient: getting or creating ceph auth key "client.rbd-mirror-peer"
2025-01-29 15:44:15.068703 I | cephclient: successfully created rbd-mirror bootstrap peer token for cluster "rook-ceph"
2025-01-29 15:44:15.110547 I | op-mgr: start running mgr
2025-01-29 15:44:15.121573 I | cephclient: getting or creating ceph auth key "mgr.a"
2025-01-29 15:44:15.706635 I | op-mgr: deployment for mgr rook-ceph-mgr-a already exists. updating if needed
2025-01-29 15:44:15.721595 I | op-k8sutil: deployment "rook-ceph-mgr-a" did not change, nothing to update
2025-01-29 15:44:15.721626 I | cephclient: getting or creating ceph auth key "mgr.b"
2025-01-29 15:44:16.324047 I | op-mgr: deployment for mgr rook-ceph-mgr-b already exists. updating if needed
2025-01-29 15:44:16.339110 I | op-k8sutil: deployment "rook-ceph-mgr-b" did not change, nothing to update
2025-01-29 15:44:16.442513 E | ceph-cluster-controller: failed to reconcile CephCluster "rook-ceph/rook-ceph". failed to reconcile cluster "rook-ceph": failed to configure local ceph cluster: failed to create cluster: failed to start ceph mgr: failed to enable mgr services: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-mgr" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:44:16.453071 I | ceph-cluster-controller: reconciling ceph cluster in namespace "rook-ceph"
2025-01-29 15:44:16.494848 I | ceph-spec: parsing mon endpoints: a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300
2025-01-29 15:44:16.725229 I | ceph-spec: detecting the ceph image version for image quay.io/ceph/ceph:v19.2.0...
2025-01-29 15:44:19.474245 I | ceph-spec: detected ceph image version: "19.2.0-0 squid"
2025-01-29 15:44:19.474279 I | ceph-cluster-controller: validating ceph version from provided image
2025-01-29 15:44:19.483154 I | ceph-spec: parsing mon endpoints: a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300
2025-01-29 15:44:19.488019 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:44:19.488351 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:44:20.023513 I | ceph-cluster-controller: cluster "rook-ceph": version "19.2.0-0 squid" detected for image "quay.io/ceph/ceph:v19.2.0"
2025-01-29 15:44:20.117864 I | op-mon: start running mons
2025-01-29 15:44:20.126556 I | ceph-spec: parsing mon endpoints: a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300
2025-01-29 15:44:20.155277 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.100.50.102:3300","10.100.50.101:3300","10.100.50.100:3300"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","radosNamespace":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":"","mirrorDaemonCount":0},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:c=10.100.50.100:3300,a=10.100.50.102:3300,b=10.100.50.101:3300 mapping:{"node":{"a":{"Name":"kubernetes-2","Hostname":"kubernetes-2","Address":"10.100.50.102"},"b":{"Name":"kubernetes-1","Hostname":"kubernetes-1","Address":"10.100.50.101"},"c":{"Name":"kubernetes-0","Hostname":"kubernetes-0","Address":"10.100.50.100"}}} maxMonId:2 outOfQuorum:]
2025-01-29 15:44:20.648048 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:44:20.648344 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:44:21.652666 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:44:22.238102 I | exec: exec timeout waiting for process rbd to return. Sending interrupt signal to the process
2025-01-29 15:44:22.254734 E | ceph-block-pool-controller: failed to reconcile CephBlockPool "rook-ceph/ceph-blockpool". failed to create pool "ceph-blockpool".: failed to configure pool "ceph-blockpool".: failed to initialize pool "ceph-blockpool" for RBD use. : signal: interrupt
2025-01-29 15:44:22.811406 I | clusterdisruption-controller: all "host" failure domains: []. osd is down in failure domain: "". active node drains: false. pg health: "cluster is not fully clean. PGs: [{StateName:unknown Count:1}]"
2025-01-29 15:44:22.817435 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2025-01-29 15:44:23.053523 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:44:23.448467 I | ceph-spec: parsing mon endpoints: c=10.100.50.100:3300,a=10.100.50.102:3300,b=10.100.50.101:3300
2025-01-29 15:44:23.849196 I | op-mon: targeting the mon count 3
2025-01-29 15:44:23.870726 I | op-config: applying ceph settings:
[global]
mon cluster log file    = 
mon allow pool size one = true
mon allow pool delete   = true
2025-01-29 15:44:23.974279 I | ceph-block-pool-controller: creating pool "ceph-blockpool" in namespace "rook-ceph"
2025-01-29 15:44:24.309791 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:44:24.310063 I | op-config: applying ceph settings:
[global]
log to file = true
2025-01-29 15:44:24.764216 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:44:24.764307 I | op-config: deleting "global" "log file" option from the mon configuration database
2025-01-29 15:44:25.187691 I | op-config: successfully deleted "log file" option from the mon configuration database
2025-01-29 15:44:25.187718 I | op-mon: checking for basic quorum with existing mons
2025-01-29 15:44:25.187741 I | op-mon: setting mon "c" endpoints for hostnetwork mode
2025-01-29 15:44:25.187754 I | op-mon: setting mon "a" endpoints for hostnetwork mode
2025-01-29 15:44:25.187763 I | op-mon: setting mon "b" endpoints for hostnetwork mode
2025-01-29 15:44:25.209239 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.100.50.102:3300","10.100.50.101:3300","10.100.50.100:3300"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","radosNamespace":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":"","mirrorDaemonCount":0},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300 mapping:{"node":{"a":{"Name":"kubernetes-2","Hostname":"kubernetes-2","Address":"10.100.50.102"},"b":{"Name":"kubernetes-1","Hostname":"kubernetes-1","Address":"10.100.50.101"},"c":{"Name":"kubernetes-0","Hostname":"kubernetes-0","Address":"10.100.50.100"}}} maxMonId:2 outOfQuorum:]
2025-01-29 15:44:25.223147 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:44:25.223384 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:44:25.471508 I | op-mon: deployment for mon rook-ceph-mon-c already exists. updating if needed
2025-01-29 15:44:25.486423 I | op-k8sutil: deployment "rook-ceph-mon-c" did not change, nothing to update
2025-01-29 15:44:25.486460 I | op-mon: waiting for mon quorum with [c a b]
2025-01-29 15:44:25.777762 I | cephclient: application "rbd" is already set on pool "ceph-blockpool"
2025-01-29 15:44:25.777797 I | cephclient: reconciling replicated pool ceph-blockpool succeeded
2025-01-29 15:44:25.777809 I | ceph-block-pool-controller: initializing pool "ceph-blockpool" for RBD use
2025-01-29 15:44:26.057820 I | op-mon: mons running: [c a b]
2025-01-29 15:44:26.642313 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:44:26.652204 I | op-mon: deployment for mon rook-ceph-mon-a already exists. updating if needed
2025-01-29 15:44:26.663659 I | op-k8sutil: deployment "rook-ceph-mon-a" did not change, nothing to update
2025-01-29 15:44:26.663681 I | op-mon: waiting for mon quorum with [c a b]
2025-01-29 15:44:26.708030 I | op-mon: mons running: [c a b]
2025-01-29 15:44:27.211241 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:44:27.219549 I | op-mon: deployment for mon rook-ceph-mon-b already exists. updating if needed
2025-01-29 15:44:27.234008 I | op-k8sutil: deployment "rook-ceph-mon-b" did not change, nothing to update
2025-01-29 15:44:27.234039 I | op-mon: waiting for mon quorum with [c a b]
2025-01-29 15:44:27.277920 I | op-mon: mons running: [c a b]
2025-01-29 15:44:27.765765 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:44:27.765788 I | op-mon: mons created: 3
2025-01-29 15:44:28.349293 I | op-mon: waiting for mon quorum with [c a b]
2025-01-29 15:44:28.394741 I | op-mon: mons running: [c a b]
2025-01-29 15:44:28.944390 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:44:28.944419 I | ceph-spec: ensuring cluster "rook-ceph" "public" network is configured to use CIDR(s) ""
2025-01-29 15:44:28.944428 I | ceph-spec: ensuring cluster "rook-ceph" "cluster" network is configured to use CIDR(s) ""
2025-01-29 15:44:28.944672 I | cephclient: getting or creating ceph auth key "client.csi-rbd-provisioner"
2025-01-29 15:44:29.652903 I | cephclient: getting or creating ceph auth key "client.csi-rbd-node"
2025-01-29 15:44:30.206582 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-provisioner"
2025-01-29 15:44:30.752455 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-node"
2025-01-29 15:44:31.318031 I | ceph-csi: created kubernetes csi secrets for cluster "rook-ceph"
2025-01-29 15:44:31.318059 I | cephclient: getting or creating ceph auth key "client.crash"
2025-01-29 15:44:31.484828 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:44:31.888486 I | ceph-nodedaemon-controller: created kubernetes crash collector secret for cluster "rook-ceph"
2025-01-29 15:44:31.888514 I | cephclient: getting or creating ceph auth key "client.ceph-exporter"
2025-01-29 15:44:32.595462 I | ceph-nodedaemon-controller: created kubernetes exporter secret for cluster "rook-ceph"
2025-01-29 15:44:32.595502 I | op-config: deleting "global" "ms_client_mode" option from the mon configuration database
2025-01-29 15:44:33.036488 I | op-config: successfully deleted "ms_client_mode" option from the mon configuration database
2025-01-29 15:44:33.036520 I | op-config: deleting "global" "rbd_default_map_options" option from the mon configuration database
2025-01-29 15:44:33.564706 I | op-config: successfully deleted "rbd_default_map_options" option from the mon configuration database
2025-01-29 15:44:33.564743 I | op-config: deleting "global" "ms_cluster_mode" option from the mon configuration database
2025-01-29 15:44:34.023606 I | op-config: successfully deleted "ms_cluster_mode" option from the mon configuration database
2025-01-29 15:44:34.023642 I | op-config: deleting "global" "ms_service_mode" option from the mon configuration database
2025-01-29 15:44:34.729279 I | clusterdisruption-controller: all "host" failure domains: []. osd is down in failure domain: "". active node drains: false. pg health: "cluster is not fully clean. PGs: [{StateName:unknown Count:1}]"
2025-01-29 15:44:34.734585 I | op-config: successfully deleted "ms_service_mode" option from the mon configuration database
2025-01-29 15:44:34.734933 I | op-config: applying ceph settings:
[global]
rbd_default_map_options = ms_mode=prefer-crc
2025-01-29 15:44:34.735517 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2025-01-29 15:44:35.178157 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:44:35.178260 I | op-config: deleting "global" "ms_osd_compress_mode" option from the mon configuration database
2025-01-29 15:44:35.282537 I | clusterdisruption-controller: all "host" failure domains: []. osd is down in failure domain: "". active node drains: false. pg health: "cluster is not fully clean. PGs: [{StateName:unknown Count:1}]"
2025-01-29 15:44:35.288941 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2025-01-29 15:44:35.581012 I | op-config: successfully deleted "ms_osd_compress_mode" option from the mon configuration database
2025-01-29 15:44:35.581044 I | cephclient: create rbd-mirror bootstrap peer token "client.rbd-mirror-peer"
2025-01-29 15:44:35.581053 I | cephclient: getting or creating ceph auth key "client.rbd-mirror-peer"
2025-01-29 15:44:36.079157 I | cephclient: successfully created rbd-mirror bootstrap peer token for cluster "rook-ceph"
2025-01-29 15:44:36.120799 I | op-mgr: start running mgr
2025-01-29 15:44:36.132142 I | cephclient: getting or creating ceph auth key "mgr.a"
2025-01-29 15:44:36.681037 I | op-mgr: deployment for mgr rook-ceph-mgr-a already exists. updating if needed
2025-01-29 15:44:36.695600 I | op-k8sutil: deployment "rook-ceph-mgr-a" did not change, nothing to update
2025-01-29 15:44:36.695630 I | cephclient: getting or creating ceph auth key "mgr.b"
2025-01-29 15:44:37.214324 I | op-mgr: deployment for mgr rook-ceph-mgr-b already exists. updating if needed
2025-01-29 15:44:37.230187 I | op-k8sutil: deployment "rook-ceph-mgr-b" did not change, nothing to update
2025-01-29 15:44:37.353607 E | ceph-cluster-controller: failed to reconcile CephCluster "rook-ceph/rook-ceph". failed to reconcile cluster "rook-ceph": failed to configure local ceph cluster: failed to create cluster: failed to start ceph mgr: failed to enable mgr services: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-mgr" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:44:37.374548 I | ceph-cluster-controller: reconciling ceph cluster in namespace "rook-ceph"
2025-01-29 15:44:37.453978 I | ceph-spec: parsing mon endpoints: a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300
2025-01-29 15:44:37.682857 I | ceph-spec: detecting the ceph image version for image quay.io/ceph/ceph:v19.2.0...
2025-01-29 15:44:39.264352 I | op-mon: checking if multiple mons are on the same node
2025-01-29 15:44:40.514617 I | ceph-spec: detected ceph image version: "19.2.0-0 squid"
2025-01-29 15:44:40.514648 I | ceph-cluster-controller: validating ceph version from provided image
2025-01-29 15:44:40.525286 I | ceph-spec: parsing mon endpoints: a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300
2025-01-29 15:44:40.530062 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:44:40.530333 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:44:40.779501 I | exec: exec timeout waiting for process rbd to return. Sending interrupt signal to the process
2025-01-29 15:44:40.795704 E | ceph-block-pool-controller: failed to reconcile CephBlockPool "rook-ceph/ceph-blockpool". failed to create pool "ceph-blockpool".: failed to configure pool "ceph-blockpool".: failed to initialize pool "ceph-blockpool" for RBD use. : signal: interrupt
2025-01-29 15:44:40.815528 I | ceph-spec: parsing mon endpoints: a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300
2025-01-29 15:44:41.228181 I | ceph-cluster-controller: cluster "rook-ceph": version "19.2.0-0 squid" detected for image "quay.io/ceph/ceph:v19.2.0"
2025-01-29 15:44:41.302172 I | ceph-block-pool-controller: creating pool "ceph-blockpool" in namespace "rook-ceph"
2025-01-29 15:44:41.339817 I | op-mon: start running mons
2025-01-29 15:44:41.348956 I | ceph-spec: parsing mon endpoints: a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300
2025-01-29 15:44:41.502269 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.100.50.102:3300","10.100.50.101:3300","10.100.50.100:3300"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","radosNamespace":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":"","mirrorDaemonCount":0},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300 mapping:{"node":{"a":{"Name":"kubernetes-2","Hostname":"kubernetes-2","Address":"10.100.50.102"},"b":{"Name":"kubernetes-1","Hostname":"kubernetes-1","Address":"10.100.50.101"},"c":{"Name":"kubernetes-0","Hostname":"kubernetes-0","Address":"10.100.50.100"}}} maxMonId:2 outOfQuorum:]
2025-01-29 15:44:42.099217 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:44:42.099541 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:44:42.633283 I | cephclient: application "rbd" is already set on pool "ceph-blockpool"
2025-01-29 15:44:42.633758 I | cephclient: reconciling replicated pool ceph-blockpool succeeded
2025-01-29 15:44:42.633901 I | ceph-block-pool-controller: initializing pool "ceph-blockpool" for RBD use
2025-01-29 15:44:43.700658 I | op-mon: targeting the mon count 3
2025-01-29 15:44:43.723478 I | op-config: applying ceph settings:
[global]
mon allow pool delete   = true
mon cluster log file    = 
mon allow pool size one = true
2025-01-29 15:44:44.160742 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:44:44.161165 I | op-config: applying ceph settings:
[global]
log to file = true
2025-01-29 15:44:44.583785 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:44:44.583869 I | op-config: deleting "global" "log file" option from the mon configuration database
2025-01-29 15:44:45.035843 I | op-config: successfully deleted "log file" option from the mon configuration database
2025-01-29 15:44:45.035875 I | op-mon: checking for basic quorum with existing mons
2025-01-29 15:44:45.035886 I | op-mon: setting mon "b" endpoints for hostnetwork mode
2025-01-29 15:44:45.035899 I | op-mon: setting mon "c" endpoints for hostnetwork mode
2025-01-29 15:44:45.035907 I | op-mon: setting mon "a" endpoints for hostnetwork mode
2025-01-29 15:44:45.055216 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.100.50.100:3300","10.100.50.102:3300","10.100.50.101:3300"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","radosNamespace":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":"","mirrorDaemonCount":0},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300 mapping:{"node":{"a":{"Name":"kubernetes-2","Hostname":"kubernetes-2","Address":"10.100.50.102"},"b":{"Name":"kubernetes-1","Hostname":"kubernetes-1","Address":"10.100.50.101"},"c":{"Name":"kubernetes-0","Hostname":"kubernetes-0","Address":"10.100.50.100"}}} maxMonId:2 outOfQuorum:]
2025-01-29 15:44:45.069693 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:44:45.070028 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:44:45.324273 I | op-mon: deployment for mon rook-ceph-mon-b already exists. updating if needed
2025-01-29 15:44:45.338935 I | op-k8sutil: deployment "rook-ceph-mon-b" did not change, nothing to update
2025-01-29 15:44:45.338978 I | op-mon: waiting for mon quorum with [b c a]
2025-01-29 15:44:45.907152 I | op-mon: mons running: [b c a]
2025-01-29 15:44:46.759419 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:44:46.768434 I | op-mon: deployment for mon rook-ceph-mon-c already exists. updating if needed
2025-01-29 15:44:46.781637 I | op-k8sutil: deployment "rook-ceph-mon-c" did not change, nothing to update
2025-01-29 15:44:46.781663 I | op-mon: waiting for mon quorum with [b c a]
2025-01-29 15:44:46.825664 I | op-mon: mons running: [b c a]
2025-01-29 15:44:47.387731 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:44:47.397393 I | op-mon: deployment for mon rook-ceph-mon-a already exists. updating if needed
2025-01-29 15:44:47.409569 I | op-k8sutil: deployment "rook-ceph-mon-a" did not change, nothing to update
2025-01-29 15:44:47.409590 I | op-mon: waiting for mon quorum with [b c a]
2025-01-29 15:44:47.454697 I | op-mon: mons running: [b c a]
2025-01-29 15:44:48.017485 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:44:48.017514 I | op-mon: mons created: 3
2025-01-29 15:44:48.584193 I | op-mon: waiting for mon quorum with [b c a]
2025-01-29 15:44:48.630182 I | op-mon: mons running: [b c a]
2025-01-29 15:44:49.281877 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:44:49.281940 I | ceph-spec: ensuring cluster "rook-ceph" "public" network is configured to use CIDR(s) ""
2025-01-29 15:44:49.281952 I | ceph-spec: ensuring cluster "rook-ceph" "cluster" network is configured to use CIDR(s) ""
2025-01-29 15:44:49.282246 I | cephclient: getting or creating ceph auth key "client.csi-rbd-provisioner"
2025-01-29 15:44:49.846449 I | cephclient: getting or creating ceph auth key "client.csi-rbd-node"
2025-01-29 15:44:50.385038 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-provisioner"
2025-01-29 15:44:50.909422 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-node"
2025-01-29 15:44:51.547070 I | ceph-csi: created kubernetes csi secrets for cluster "rook-ceph"
2025-01-29 15:44:51.547100 I | cephclient: getting or creating ceph auth key "client.crash"
2025-01-29 15:44:52.390924 I | ceph-nodedaemon-controller: created kubernetes crash collector secret for cluster "rook-ceph"
2025-01-29 15:44:52.390955 I | cephclient: getting or creating ceph auth key "client.ceph-exporter"
2025-01-29 15:44:53.147192 I | ceph-nodedaemon-controller: created kubernetes exporter secret for cluster "rook-ceph"
2025-01-29 15:44:53.147237 I | op-config: deleting "global" "ms_cluster_mode" option from the mon configuration database
2025-01-29 15:44:53.564552 I | op-config: successfully deleted "ms_cluster_mode" option from the mon configuration database
2025-01-29 15:44:53.564577 I | op-config: deleting "global" "ms_service_mode" option from the mon configuration database
2025-01-29 15:44:53.984731 I | op-config: successfully deleted "ms_service_mode" option from the mon configuration database
2025-01-29 15:44:53.984762 I | op-config: deleting "global" "ms_client_mode" option from the mon configuration database
2025-01-29 15:44:54.467084 I | op-config: successfully deleted "ms_client_mode" option from the mon configuration database
2025-01-29 15:44:54.467113 I | op-config: deleting "global" "rbd_default_map_options" option from the mon configuration database
2025-01-29 15:44:55.122063 I | op-config: successfully deleted "rbd_default_map_options" option from the mon configuration database
2025-01-29 15:44:55.122476 I | op-config: applying ceph settings:
[global]
rbd_default_map_options = ms_mode=prefer-crc
2025-01-29 15:44:55.546962 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:44:55.547087 I | op-config: deleting "global" "ms_osd_compress_mode" option from the mon configuration database
2025-01-29 15:44:56.261415 I | op-config: successfully deleted "ms_osd_compress_mode" option from the mon configuration database
2025-01-29 15:44:56.261449 I | cephclient: create rbd-mirror bootstrap peer token "client.rbd-mirror-peer"
2025-01-29 15:44:56.261456 I | cephclient: getting or creating ceph auth key "client.rbd-mirror-peer"
2025-01-29 15:44:56.788986 I | cephclient: successfully created rbd-mirror bootstrap peer token for cluster "rook-ceph"
2025-01-29 15:44:56.831008 I | op-mgr: start running mgr
2025-01-29 15:44:56.841941 I | cephclient: getting or creating ceph auth key "mgr.a"
2025-01-29 15:44:57.482533 I | op-mgr: deployment for mgr rook-ceph-mgr-a already exists. updating if needed
2025-01-29 15:44:57.497482 I | op-k8sutil: deployment "rook-ceph-mgr-a" did not change, nothing to update
2025-01-29 15:44:57.497511 I | cephclient: getting or creating ceph auth key "mgr.b"
2025-01-29 15:44:57.639116 I | exec: exec timeout waiting for process rbd to return. Sending interrupt signal to the process
2025-01-29 15:44:57.657000 E | ceph-block-pool-controller: failed to reconcile CephBlockPool "rook-ceph/ceph-blockpool". failed to create pool "ceph-blockpool".: failed to configure pool "ceph-blockpool".: failed to initialize pool "ceph-blockpool" for RBD use. : signal: interrupt
2025-01-29 15:44:57.687064 I | ceph-spec: parsing mon endpoints: a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300
2025-01-29 15:44:58.167307 I | ceph-block-pool-controller: creating pool "ceph-blockpool" in namespace "rook-ceph"
2025-01-29 15:44:58.169499 I | op-mgr: deployment for mgr rook-ceph-mgr-b already exists. updating if needed
2025-01-29 15:44:58.181387 I | op-k8sutil: deployment "rook-ceph-mgr-b" did not change, nothing to update
2025-01-29 15:44:58.283916 E | ceph-cluster-controller: failed to reconcile CephCluster "rook-ceph/rook-ceph". failed to reconcile cluster "rook-ceph": failed to configure local ceph cluster: failed to create cluster: failed to start ceph mgr: failed to enable mgr services: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-mgr" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:44:58.325105 I | ceph-cluster-controller: reconciling ceph cluster in namespace "rook-ceph"
2025-01-29 15:44:58.652512 I | ceph-spec: parsing mon endpoints: a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300
2025-01-29 15:44:58.881537 I | ceph-spec: detecting the ceph image version for image quay.io/ceph/ceph:v19.2.0...
2025-01-29 15:44:59.517098 I | cephclient: application "rbd" is already set on pool "ceph-blockpool"
2025-01-29 15:44:59.517124 I | cephclient: reconciling replicated pool ceph-blockpool succeeded
2025-01-29 15:44:59.517134 I | ceph-block-pool-controller: initializing pool "ceph-blockpool" for RBD use
2025-01-29 15:45:01.660611 I | ceph-spec: detected ceph image version: "19.2.0-0 squid"
2025-01-29 15:45:01.660639 I | ceph-cluster-controller: validating ceph version from provided image
2025-01-29 15:45:01.669251 I | ceph-spec: parsing mon endpoints: a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300
2025-01-29 15:45:01.673906 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:45:01.674270 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:45:02.178212 I | ceph-cluster-controller: cluster "rook-ceph": version "19.2.0-0 squid" detected for image "quay.io/ceph/ceph:v19.2.0"
2025-01-29 15:45:02.275314 I | op-mon: start running mons
2025-01-29 15:45:02.283978 I | ceph-spec: parsing mon endpoints: a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300
2025-01-29 15:45:02.315622 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.100.50.102:3300","10.100.50.101:3300","10.100.50.100:3300"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","radosNamespace":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":"","mirrorDaemonCount":0},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300 mapping:{"node":{"a":{"Name":"kubernetes-2","Hostname":"kubernetes-2","Address":"10.100.50.102"},"b":{"Name":"kubernetes-1","Hostname":"kubernetes-1","Address":"10.100.50.101"},"c":{"Name":"kubernetes-0","Hostname":"kubernetes-0","Address":"10.100.50.100"}}} maxMonId:2 outOfQuorum:]
2025-01-29 15:45:02.843220 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:45:02.843531 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:45:03.847506 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:45:05.248074 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:45:05.255661 I | clusterdisruption-controller: all "host" failure domains: []. osd is down in failure domain: "". active node drains: false. pg health: "cluster is not fully clean. PGs: [{StateName:unknown Count:1}]"
2025-01-29 15:45:05.261781 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2025-01-29 15:45:05.643721 I | op-mon: targeting the mon count 3
2025-01-29 15:45:05.665743 I | op-config: applying ceph settings:
[global]
mon allow pool size one = true
mon allow pool delete   = true
mon cluster log file    = 
2025-01-29 15:45:05.807756 I | clusterdisruption-controller: all "host" failure domains: []. osd is down in failure domain: "". active node drains: false. pg health: "cluster is not fully clean. PGs: [{StateName:unknown Count:1}]"
2025-01-29 15:45:05.813777 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2025-01-29 15:45:06.088749 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:45:06.089215 I | op-config: applying ceph settings:
[global]
log to file = true
2025-01-29 15:45:06.511413 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:45:06.511500 I | op-config: deleting "global" "log file" option from the mon configuration database
2025-01-29 15:45:07.134202 I | op-config: successfully deleted "log file" option from the mon configuration database
2025-01-29 15:45:07.134233 I | op-mon: checking for basic quorum with existing mons
2025-01-29 15:45:07.134245 I | op-mon: setting mon "a" endpoints for hostnetwork mode
2025-01-29 15:45:07.134258 I | op-mon: setting mon "b" endpoints for hostnetwork mode
2025-01-29 15:45:07.134266 I | op-mon: setting mon "c" endpoints for hostnetwork mode
2025-01-29 15:45:07.153663 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.100.50.102:3300","10.100.50.101:3300","10.100.50.100:3300"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","radosNamespace":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":"","mirrorDaemonCount":0},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300 mapping:{"node":{"a":{"Name":"kubernetes-2","Hostname":"kubernetes-2","Address":"10.100.50.102"},"b":{"Name":"kubernetes-1","Hostname":"kubernetes-1","Address":"10.100.50.101"},"c":{"Name":"kubernetes-0","Hostname":"kubernetes-0","Address":"10.100.50.100"}}} maxMonId:2 outOfQuorum:]
2025-01-29 15:45:07.167101 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:45:07.167393 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:45:07.265552 I | op-mon: deployment for mon rook-ceph-mon-a already exists. updating if needed
2025-01-29 15:45:07.279386 I | op-k8sutil: deployment "rook-ceph-mon-a" did not change, nothing to update
2025-01-29 15:45:07.279420 I | op-mon: waiting for mon quorum with [a b c]
2025-01-29 15:45:07.853728 I | op-mon: mons running: [a b c]
2025-01-29 15:45:08.430305 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:45:08.441364 I | op-mon: deployment for mon rook-ceph-mon-b already exists. updating if needed
2025-01-29 15:45:08.452190 I | op-k8sutil: deployment "rook-ceph-mon-b" did not change, nothing to update
2025-01-29 15:45:08.452210 I | op-mon: waiting for mon quorum with [a b c]
2025-01-29 15:45:08.496183 I | op-mon: mons running: [a b c]
2025-01-29 15:45:09.048375 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:45:09.057067 I | op-mon: deployment for mon rook-ceph-mon-c already exists. updating if needed
2025-01-29 15:45:09.068066 I | op-k8sutil: deployment "rook-ceph-mon-c" did not change, nothing to update
2025-01-29 15:45:09.068087 I | op-mon: waiting for mon quorum with [a b c]
2025-01-29 15:45:09.112033 I | op-mon: mons running: [a b c]
2025-01-29 15:45:09.945728 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:45:09.945762 I | op-mon: mons created: 3
2025-01-29 15:45:10.617264 I | op-mon: waiting for mon quorum with [a b c]
2025-01-29 15:45:10.662571 I | op-mon: mons running: [a b c]
2025-01-29 15:45:11.187745 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:45:11.187780 I | ceph-spec: ensuring cluster "rook-ceph" "public" network is configured to use CIDR(s) ""
2025-01-29 15:45:11.187787 I | ceph-spec: ensuring cluster "rook-ceph" "cluster" network is configured to use CIDR(s) ""
2025-01-29 15:45:11.187996 I | cephclient: getting or creating ceph auth key "client.csi-rbd-provisioner"
2025-01-29 15:45:11.785094 I | cephclient: getting or creating ceph auth key "client.csi-rbd-node"
2025-01-29 15:45:12.322495 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-provisioner"
2025-01-29 15:45:12.521246 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:45:12.899705 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-node"
2025-01-29 15:45:13.770333 I | ceph-csi: created kubernetes csi secrets for cluster "rook-ceph"
2025-01-29 15:45:13.770379 I | cephclient: getting or creating ceph auth key "client.crash"
2025-01-29 15:45:14.366448 I | ceph-nodedaemon-controller: created kubernetes crash collector secret for cluster "rook-ceph"
2025-01-29 15:45:14.366482 I | cephclient: getting or creating ceph auth key "client.ceph-exporter"
2025-01-29 15:45:14.518073 I | exec: exec timeout waiting for process rbd to return. Sending interrupt signal to the process
2025-01-29 15:45:14.532780 E | ceph-block-pool-controller: failed to reconcile CephBlockPool "rook-ceph/ceph-blockpool". failed to create pool "ceph-blockpool".: failed to configure pool "ceph-blockpool".: failed to initialize pool "ceph-blockpool" for RBD use. : signal: interrupt
2025-01-29 15:45:14.582310 I | ceph-spec: parsing mon endpoints: a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300
2025-01-29 15:45:15.046412 I | ceph-block-pool-controller: creating pool "ceph-blockpool" in namespace "rook-ceph"
2025-01-29 15:45:15.212380 I | ceph-nodedaemon-controller: created kubernetes exporter secret for cluster "rook-ceph"
2025-01-29 15:45:15.212415 I | op-config: deleting "global" "ms_cluster_mode" option from the mon configuration database
2025-01-29 15:45:15.638171 I | op-config: successfully deleted "ms_cluster_mode" option from the mon configuration database
2025-01-29 15:45:15.638194 I | op-config: deleting "global" "ms_service_mode" option from the mon configuration database
2025-01-29 15:45:16.168851 I | op-config: successfully deleted "ms_service_mode" option from the mon configuration database
2025-01-29 15:45:16.168914 I | op-config: deleting "global" "ms_client_mode" option from the mon configuration database
2025-01-29 15:45:16.486922 I | cephclient: application "rbd" is already set on pool "ceph-blockpool"
2025-01-29 15:45:16.486956 I | cephclient: reconciling replicated pool ceph-blockpool succeeded
2025-01-29 15:45:16.486968 I | ceph-block-pool-controller: initializing pool "ceph-blockpool" for RBD use
2025-01-29 15:45:16.638468 I | op-config: successfully deleted "ms_client_mode" option from the mon configuration database
2025-01-29 15:45:16.638505 I | op-config: deleting "global" "rbd_default_map_options" option from the mon configuration database
2025-01-29 15:45:17.096640 I | op-config: successfully deleted "rbd_default_map_options" option from the mon configuration database
2025-01-29 15:45:17.096962 I | op-config: applying ceph settings:
[global]
rbd_default_map_options = ms_mode=prefer-crc
2025-01-29 15:45:17.538027 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:45:17.538137 I | op-config: deleting "global" "ms_osd_compress_mode" option from the mon configuration database
2025-01-29 15:45:17.987314 I | op-config: successfully deleted "ms_osd_compress_mode" option from the mon configuration database
2025-01-29 15:45:17.987362 I | cephclient: create rbd-mirror bootstrap peer token "client.rbd-mirror-peer"
2025-01-29 15:45:17.987370 I | cephclient: getting or creating ceph auth key "client.rbd-mirror-peer"
2025-01-29 15:45:18.565723 I | cephclient: successfully created rbd-mirror bootstrap peer token for cluster "rook-ceph"
2025-01-29 15:45:18.607357 I | op-mgr: start running mgr
2025-01-29 15:45:18.618214 I | cephclient: getting or creating ceph auth key "mgr.a"
2025-01-29 15:45:19.282939 I | op-mgr: deployment for mgr rook-ceph-mgr-a already exists. updating if needed
2025-01-29 15:45:19.297137 I | op-k8sutil: deployment "rook-ceph-mgr-a" did not change, nothing to update
2025-01-29 15:45:19.297167 I | cephclient: getting or creating ceph auth key "mgr.b"
2025-01-29 15:45:19.933648 I | op-mgr: deployment for mgr rook-ceph-mgr-b already exists. updating if needed
2025-01-29 15:45:19.948620 I | op-k8sutil: deployment "rook-ceph-mgr-b" did not change, nothing to update
2025-01-29 15:45:20.052612 E | ceph-cluster-controller: failed to reconcile CephCluster "rook-ceph/rook-ceph". failed to reconcile cluster "rook-ceph": failed to configure local ceph cluster: failed to create cluster: failed to start ceph mgr: failed to enable mgr services: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-mgr" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:45:20.133636 I | ceph-cluster-controller: reconciling ceph cluster in namespace "rook-ceph"
2025-01-29 15:45:20.142638 I | ceph-spec: parsing mon endpoints: a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300
2025-01-29 15:45:20.335779 I | ceph-spec: detecting the ceph image version for image quay.io/ceph/ceph:v19.2.0...
2025-01-29 15:45:22.836389 I | ceph-spec: detected ceph image version: "19.2.0-0 squid"
2025-01-29 15:45:22.836414 I | ceph-cluster-controller: validating ceph version from provided image
2025-01-29 15:45:22.846360 I | ceph-spec: parsing mon endpoints: a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300
2025-01-29 15:45:22.850897 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:45:22.851191 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:45:23.402328 I | ceph-cluster-controller: cluster "rook-ceph": version "19.2.0-0 squid" detected for image "quay.io/ceph/ceph:v19.2.0"
2025-01-29 15:45:23.501283 I | op-mon: start running mons
2025-01-29 15:45:23.509805 I | ceph-spec: parsing mon endpoints: a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300
2025-01-29 15:45:23.540304 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.100.50.102:3300","10.100.50.101:3300","10.100.50.100:3300"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","radosNamespace":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":"","mirrorDaemonCount":0},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:b=10.100.50.101:3300,c=10.100.50.100:3300,a=10.100.50.102:3300 mapping:{"node":{"a":{"Name":"kubernetes-2","Hostname":"kubernetes-2","Address":"10.100.50.102"},"b":{"Name":"kubernetes-1","Hostname":"kubernetes-1","Address":"10.100.50.101"},"c":{"Name":"kubernetes-0","Hostname":"kubernetes-0","Address":"10.100.50.100"}}} maxMonId:2 outOfQuorum:]
2025-01-29 15:45:24.017496 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:45:24.017700 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:45:25.618136 I | op-mon: targeting the mon count 3
2025-01-29 15:45:25.639387 I | op-config: applying ceph settings:
[global]
mon allow pool delete   = true
mon cluster log file    = 
mon allow pool size one = true
2025-01-29 15:45:26.341023 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:45:26.341365 I | op-config: applying ceph settings:
[global]
log to file = true
2025-01-29 15:45:26.805933 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:45:26.806046 I | op-config: deleting "global" "log file" option from the mon configuration database
2025-01-29 15:45:27.229899 I | op-config: successfully deleted "log file" option from the mon configuration database
2025-01-29 15:45:27.229931 I | op-mon: checking for basic quorum with existing mons
2025-01-29 15:45:27.229943 I | op-mon: setting mon "c" endpoints for hostnetwork mode
2025-01-29 15:45:27.229956 I | op-mon: setting mon "a" endpoints for hostnetwork mode
2025-01-29 15:45:27.229964 I | op-mon: setting mon "b" endpoints for hostnetwork mode
2025-01-29 15:45:27.250073 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.100.50.102:3300","10.100.50.101:3300","10.100.50.100:3300"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","radosNamespace":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":"","mirrorDaemonCount":0},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300 mapping:{"node":{"a":{"Name":"kubernetes-2","Hostname":"kubernetes-2","Address":"10.100.50.102"},"b":{"Name":"kubernetes-1","Hostname":"kubernetes-1","Address":"10.100.50.101"},"c":{"Name":"kubernetes-0","Hostname":"kubernetes-0","Address":"10.100.50.100"}}} maxMonId:2 outOfQuorum:]
2025-01-29 15:45:27.264784 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:45:27.265044 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:45:27.294753 I | op-mon: deployment for mon rook-ceph-mon-c already exists. updating if needed
2025-01-29 15:45:27.308460 I | op-k8sutil: deployment "rook-ceph-mon-c" did not change, nothing to update
2025-01-29 15:45:27.308491 I | op-mon: waiting for mon quorum with [c a b]
2025-01-29 15:45:27.827920 I | op-mon: mons running: [c a b]
2025-01-29 15:45:28.410704 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:45:28.419368 I | op-mon: deployment for mon rook-ceph-mon-a already exists. updating if needed
2025-01-29 15:45:28.430691 I | op-k8sutil: deployment "rook-ceph-mon-a" did not change, nothing to update
2025-01-29 15:45:28.430711 I | op-mon: waiting for mon quorum with [c a b]
2025-01-29 15:45:28.492204 I | op-mon: mons running: [c a b]
2025-01-29 15:45:29.329570 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:45:29.337925 I | op-mon: deployment for mon rook-ceph-mon-b already exists. updating if needed
2025-01-29 15:45:29.349021 I | op-k8sutil: deployment "rook-ceph-mon-b" did not change, nothing to update
2025-01-29 15:45:29.349043 I | op-mon: waiting for mon quorum with [c a b]
2025-01-29 15:45:29.393153 I | op-mon: mons running: [c a b]
2025-01-29 15:45:30.153885 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:45:30.153921 I | op-mon: mons created: 3
2025-01-29 15:45:30.810305 I | op-mon: waiting for mon quorum with [c a b]
2025-01-29 15:45:30.854684 I | op-mon: mons running: [c a b]
2025-01-29 15:45:31.488503 I | exec: exec timeout waiting for process rbd to return. Sending interrupt signal to the process
2025-01-29 15:45:31.504792 E | ceph-block-pool-controller: failed to reconcile CephBlockPool "rook-ceph/ceph-blockpool". failed to create pool "ceph-blockpool".: failed to configure pool "ceph-blockpool".: failed to initialize pool "ceph-blockpool" for RBD use. : signal: interrupt
2025-01-29 15:45:31.516670 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:45:31.516699 I | ceph-spec: ensuring cluster "rook-ceph" "public" network is configured to use CIDR(s) ""
2025-01-29 15:45:31.516710 I | ceph-spec: ensuring cluster "rook-ceph" "cluster" network is configured to use CIDR(s) ""
2025-01-29 15:45:31.516991 I | cephclient: getting or creating ceph auth key "client.csi-rbd-provisioner"
2025-01-29 15:45:31.595684 I | ceph-spec: parsing mon endpoints: a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300
2025-01-29 15:45:32.045326 I | ceph-block-pool-controller: creating pool "ceph-blockpool" in namespace "rook-ceph"
2025-01-29 15:45:32.130445 I | cephclient: getting or creating ceph auth key "client.csi-rbd-node"
2025-01-29 15:45:32.699452 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-provisioner"
2025-01-29 15:45:33.236894 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-node"
2025-01-29 15:45:33.477441 I | cephclient: application "rbd" is already set on pool "ceph-blockpool"
2025-01-29 15:45:33.477494 I | cephclient: reconciling replicated pool ceph-blockpool succeeded
2025-01-29 15:45:33.477512 I | ceph-block-pool-controller: initializing pool "ceph-blockpool" for RBD use
2025-01-29 15:45:33.897725 I | ceph-csi: created kubernetes csi secrets for cluster "rook-ceph"
2025-01-29 15:45:33.897754 I | cephclient: getting or creating ceph auth key "client.crash"
2025-01-29 15:45:34.646541 I | ceph-nodedaemon-controller: created kubernetes crash collector secret for cluster "rook-ceph"
2025-01-29 15:45:34.646575 I | cephclient: getting or creating ceph auth key "client.ceph-exporter"
2025-01-29 15:45:35.243194 I | ceph-nodedaemon-controller: created kubernetes exporter secret for cluster "rook-ceph"
2025-01-29 15:45:35.243226 I | op-config: deleting "global" "rbd_default_map_options" option from the mon configuration database
2025-01-29 15:45:35.678515 I | op-config: successfully deleted "rbd_default_map_options" option from the mon configuration database
2025-01-29 15:45:35.678543 I | op-config: deleting "global" "ms_cluster_mode" option from the mon configuration database
2025-01-29 15:45:35.852359 I | clusterdisruption-controller: all "host" failure domains: []. osd is down in failure domain: "". active node drains: false. pg health: "cluster is not fully clean. PGs: [{StateName:unknown Count:1}]"
2025-01-29 15:45:35.858544 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2025-01-29 15:45:36.106880 I | op-config: successfully deleted "ms_cluster_mode" option from the mon configuration database
2025-01-29 15:45:36.106900 I | op-config: deleting "global" "ms_service_mode" option from the mon configuration database
2025-01-29 15:45:36.446533 I | clusterdisruption-controller: all "host" failure domains: []. osd is down in failure domain: "". active node drains: false. pg health: "cluster is not fully clean. PGs: [{StateName:unknown Count:1}]"
2025-01-29 15:45:36.452616 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2025-01-29 15:45:36.602820 I | op-config: successfully deleted "ms_service_mode" option from the mon configuration database
2025-01-29 15:45:36.602841 I | op-config: deleting "global" "ms_client_mode" option from the mon configuration database
2025-01-29 15:45:37.102819 I | op-config: successfully deleted "ms_client_mode" option from the mon configuration database
2025-01-29 15:45:37.103083 I | op-config: applying ceph settings:
[global]
rbd_default_map_options = ms_mode=prefer-crc
2025-01-29 15:45:37.604980 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:45:37.605084 I | op-config: deleting "global" "ms_osd_compress_mode" option from the mon configuration database
2025-01-29 15:45:38.037004 I | op-config: successfully deleted "ms_osd_compress_mode" option from the mon configuration database
2025-01-29 15:45:38.037041 I | cephclient: create rbd-mirror bootstrap peer token "client.rbd-mirror-peer"
2025-01-29 15:45:38.037051 I | cephclient: getting or creating ceph auth key "client.rbd-mirror-peer"
2025-01-29 15:45:38.635506 I | cephclient: successfully created rbd-mirror bootstrap peer token for cluster "rook-ceph"
2025-01-29 15:45:38.678021 I | op-mgr: start running mgr
2025-01-29 15:45:38.688894 I | cephclient: getting or creating ceph auth key "mgr.a"
2025-01-29 15:45:39.327608 I | op-mgr: deployment for mgr rook-ceph-mgr-a already exists. updating if needed
2025-01-29 15:45:39.346221 I | op-k8sutil: deployment "rook-ceph-mgr-a" did not change, nothing to update
2025-01-29 15:45:39.346253 I | cephclient: getting or creating ceph auth key "mgr.b"
2025-01-29 15:45:40.129819 I | op-mgr: deployment for mgr rook-ceph-mgr-b already exists. updating if needed
2025-01-29 15:45:40.146676 I | op-k8sutil: deployment "rook-ceph-mgr-b" did not change, nothing to update
2025-01-29 15:45:40.260934 E | ceph-cluster-controller: failed to reconcile CephCluster "rook-ceph/rook-ceph". failed to reconcile cluster "rook-ceph": failed to configure local ceph cluster: failed to create cluster: failed to start ceph mgr: failed to enable mgr services: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-mgr" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:45:40.421416 I | ceph-cluster-controller: reconciling ceph cluster in namespace "rook-ceph"
2025-01-29 15:45:40.430343 I | ceph-spec: parsing mon endpoints: a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300
2025-01-29 15:45:40.535223 I | ceph-spec: detecting the ceph image version for image quay.io/ceph/ceph:v19.2.0...
2025-01-29 15:45:42.917204 I | ceph-spec: detected ceph image version: "19.2.0-0 squid"
2025-01-29 15:45:42.917229 I | ceph-cluster-controller: validating ceph version from provided image
2025-01-29 15:45:42.926542 I | ceph-spec: parsing mon endpoints: a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300
2025-01-29 15:45:42.931440 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:45:42.931750 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:45:43.782660 I | ceph-cluster-controller: cluster "rook-ceph": version "19.2.0-0 squid" detected for image "quay.io/ceph/ceph:v19.2.0"
2025-01-29 15:45:43.903993 I | op-mon: start running mons
2025-01-29 15:45:43.913221 I | ceph-spec: parsing mon endpoints: a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300
2025-01-29 15:45:43.944173 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.100.50.100:3300","10.100.50.102:3300","10.100.50.101:3300"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","radosNamespace":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":"","mirrorDaemonCount":0},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300 mapping:{"node":{"a":{"Name":"kubernetes-2","Hostname":"kubernetes-2","Address":"10.100.50.102"},"b":{"Name":"kubernetes-1","Hostname":"kubernetes-1","Address":"10.100.50.101"},"c":{"Name":"kubernetes-0","Hostname":"kubernetes-0","Address":"10.100.50.100"}}} maxMonId:2 outOfQuorum:]
2025-01-29 15:45:44.103232 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:45:44.103510 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:45:45.705067 I | op-mon: targeting the mon count 3
2025-01-29 15:45:45.727183 I | op-config: applying ceph settings:
[global]
mon cluster log file    = 
mon allow pool size one = true
mon allow pool delete   = true
2025-01-29 15:45:46.159547 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:45:46.159941 I | op-config: applying ceph settings:
[global]
log to file = true
2025-01-29 15:45:46.619563 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:45:46.619673 I | op-config: deleting "global" "log file" option from the mon configuration database
2025-01-29 15:45:47.054189 I | op-config: successfully deleted "log file" option from the mon configuration database
2025-01-29 15:45:47.054218 I | op-mon: checking for basic quorum with existing mons
2025-01-29 15:45:47.054240 I | op-mon: setting mon "a" endpoints for hostnetwork mode
2025-01-29 15:45:47.054254 I | op-mon: setting mon "b" endpoints for hostnetwork mode
2025-01-29 15:45:47.054271 I | op-mon: setting mon "c" endpoints for hostnetwork mode
2025-01-29 15:45:47.074327 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.100.50.102:3300","10.100.50.101:3300","10.100.50.100:3300"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","radosNamespace":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":"","mirrorDaemonCount":0},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:b=10.100.50.101:3300,c=10.100.50.100:3300,a=10.100.50.102:3300 mapping:{"node":{"a":{"Name":"kubernetes-2","Hostname":"kubernetes-2","Address":"10.100.50.102"},"b":{"Name":"kubernetes-1","Hostname":"kubernetes-1","Address":"10.100.50.101"},"c":{"Name":"kubernetes-0","Hostname":"kubernetes-0","Address":"10.100.50.100"}}} maxMonId:2 outOfQuorum:]
2025-01-29 15:45:47.088611 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:45:47.088949 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:45:47.327266 I | op-mon: deployment for mon rook-ceph-mon-a already exists. updating if needed
2025-01-29 15:45:47.341767 I | op-k8sutil: deployment "rook-ceph-mon-a" did not change, nothing to update
2025-01-29 15:45:47.341803 I | op-mon: waiting for mon quorum with [a b c]
2025-01-29 15:45:47.906212 I | op-mon: mons running: [a b c]
2025-01-29 15:45:48.478507 I | exec: exec timeout waiting for process rbd to return. Sending interrupt signal to the process
2025-01-29 15:45:48.493956 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:45:48.494260 E | ceph-block-pool-controller: failed to reconcile CephBlockPool "rook-ceph/ceph-blockpool". failed to create pool "ceph-blockpool".: failed to configure pool "ceph-blockpool".: failed to initialize pool "ceph-blockpool" for RBD use. : signal: interrupt
2025-01-29 15:45:48.502256 I | op-mon: deployment for mon rook-ceph-mon-b already exists. updating if needed
2025-01-29 15:45:48.514306 I | op-k8sutil: deployment "rook-ceph-mon-b" did not change, nothing to update
2025-01-29 15:45:48.514328 I | op-mon: waiting for mon quorum with [a b c]
2025-01-29 15:45:48.558775 I | op-mon: mons running: [a b c]
2025-01-29 15:45:48.903008 I | ceph-spec: parsing mon endpoints: b=10.100.50.101:3300,c=10.100.50.100:3300,a=10.100.50.102:3300
2025-01-29 15:45:49.082846 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:45:49.090497 I | op-mon: deployment for mon rook-ceph-mon-c already exists. updating if needed
2025-01-29 15:45:49.104429 I | op-k8sutil: deployment "rook-ceph-mon-c" did not change, nothing to update
2025-01-29 15:45:49.104462 I | op-mon: waiting for mon quorum with [a b c]
2025-01-29 15:45:49.490555 I | ceph-block-pool-controller: creating pool "ceph-blockpool" in namespace "rook-ceph"
2025-01-29 15:45:49.512891 I | op-mon: mons running: [a b c]
2025-01-29 15:45:50.060367 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:45:50.060394 I | op-mon: mons created: 3
2025-01-29 15:45:50.595025 I | op-mon: waiting for mon quorum with [a b c]
2025-01-29 15:45:50.640897 I | op-mon: mons running: [a b c]
2025-01-29 15:45:51.029560 I | cephclient: application "rbd" is already set on pool "ceph-blockpool"
2025-01-29 15:45:51.029594 I | cephclient: reconciling replicated pool ceph-blockpool succeeded
2025-01-29 15:45:51.029605 I | ceph-block-pool-controller: initializing pool "ceph-blockpool" for RBD use
2025-01-29 15:45:51.158110 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:45:51.158153 I | ceph-spec: ensuring cluster "rook-ceph" "public" network is configured to use CIDR(s) ""
2025-01-29 15:45:51.158163 I | ceph-spec: ensuring cluster "rook-ceph" "cluster" network is configured to use CIDR(s) ""
2025-01-29 15:45:51.158501 I | cephclient: getting or creating ceph auth key "client.csi-rbd-provisioner"
2025-01-29 15:45:51.699508 I | cephclient: getting or creating ceph auth key "client.csi-rbd-node"
2025-01-29 15:45:52.299770 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-provisioner"
2025-01-29 15:45:52.924802 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-node"
2025-01-29 15:45:53.529821 I | ceph-csi: created kubernetes csi secrets for cluster "rook-ceph"
2025-01-29 15:45:53.529848 I | cephclient: getting or creating ceph auth key "client.crash"
2025-01-29 15:45:54.140782 I | ceph-nodedaemon-controller: created kubernetes crash collector secret for cluster "rook-ceph"
2025-01-29 15:45:54.140807 I | cephclient: getting or creating ceph auth key "client.ceph-exporter"
2025-01-29 15:45:54.723355 I | ceph-nodedaemon-controller: created kubernetes exporter secret for cluster "rook-ceph"
2025-01-29 15:45:54.723391 I | op-config: deleting "global" "ms_cluster_mode" option from the mon configuration database
2025-01-29 15:45:55.132376 I | op-config: successfully deleted "ms_cluster_mode" option from the mon configuration database
2025-01-29 15:45:55.132398 I | op-config: deleting "global" "ms_service_mode" option from the mon configuration database
2025-01-29 15:45:55.590050 I | op-config: successfully deleted "ms_service_mode" option from the mon configuration database
2025-01-29 15:45:55.590067 I | op-config: deleting "global" "ms_client_mode" option from the mon configuration database
2025-01-29 15:45:56.020742 I | op-config: successfully deleted "ms_client_mode" option from the mon configuration database
2025-01-29 15:45:56.020765 I | op-config: deleting "global" "rbd_default_map_options" option from the mon configuration database
2025-01-29 15:45:56.471157 I | op-config: successfully deleted "rbd_default_map_options" option from the mon configuration database
2025-01-29 15:45:56.471517 I | op-config: applying ceph settings:
[global]
rbd_default_map_options = ms_mode=prefer-crc
2025-01-29 15:45:57.083555 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:45:57.083643 I | op-config: deleting "global" "ms_osd_compress_mode" option from the mon configuration database
2025-01-29 15:45:57.486493 I | op-config: successfully deleted "ms_osd_compress_mode" option from the mon configuration database
2025-01-29 15:45:57.486527 I | cephclient: create rbd-mirror bootstrap peer token "client.rbd-mirror-peer"
2025-01-29 15:45:57.486540 I | cephclient: getting or creating ceph auth key "client.rbd-mirror-peer"
2025-01-29 15:45:58.010115 I | cephclient: successfully created rbd-mirror bootstrap peer token for cluster "rook-ceph"
2025-01-29 15:45:58.051144 I | op-mgr: start running mgr
2025-01-29 15:45:58.061803 I | cephclient: getting or creating ceph auth key "mgr.a"
2025-01-29 15:45:58.682788 I | op-mgr: deployment for mgr rook-ceph-mgr-a already exists. updating if needed
2025-01-29 15:45:58.698599 I | op-k8sutil: deployment "rook-ceph-mgr-a" did not change, nothing to update
2025-01-29 15:45:58.698631 I | cephclient: getting or creating ceph auth key "mgr.b"
2025-01-29 15:45:59.247281 I | op-mgr: deployment for mgr rook-ceph-mgr-b already exists. updating if needed
2025-01-29 15:45:59.262100 I | op-k8sutil: deployment "rook-ceph-mgr-b" did not change, nothing to update
2025-01-29 15:45:59.362639 E | ceph-cluster-controller: failed to reconcile CephCluster "rook-ceph/rook-ceph". failed to reconcile cluster "rook-ceph": failed to configure local ceph cluster: failed to create cluster: failed to start ceph mgr: failed to enable mgr services: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-mgr" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:45:59.683155 I | ceph-cluster-controller: reconciling ceph cluster in namespace "rook-ceph"
2025-01-29 15:45:59.692484 I | ceph-spec: parsing mon endpoints: b=10.100.50.101:3300,c=10.100.50.100:3300,a=10.100.50.102:3300
2025-01-29 15:45:59.737755 I | ceph-spec: detecting the ceph image version for image quay.io/ceph/ceph:v19.2.0...
2025-01-29 15:46:01.966935 I | ceph-spec: detected ceph image version: "19.2.0-0 squid"
2025-01-29 15:46:01.966964 I | ceph-cluster-controller: validating ceph version from provided image
2025-01-29 15:46:01.976743 I | ceph-spec: parsing mon endpoints: b=10.100.50.101:3300,c=10.100.50.100:3300,a=10.100.50.102:3300
2025-01-29 15:46:01.981537 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:46:01.981717 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:46:02.966063 I | ceph-cluster-controller: cluster "rook-ceph": version "19.2.0-0 squid" detected for image "quay.io/ceph/ceph:v19.2.0"
2025-01-29 15:46:03.067813 I | op-mon: start running mons
2025-01-29 15:46:03.079220 I | ceph-spec: parsing mon endpoints: b=10.100.50.101:3300,c=10.100.50.100:3300,a=10.100.50.102:3300
2025-01-29 15:46:03.109016 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.100.50.100:3300","10.100.50.102:3300","10.100.50.101:3300"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","radosNamespace":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":"","mirrorDaemonCount":0},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:b=10.100.50.101:3300,c=10.100.50.100:3300,a=10.100.50.102:3300 mapping:{"node":{"a":{"Name":"kubernetes-2","Hostname":"kubernetes-2","Address":"10.100.50.102"},"b":{"Name":"kubernetes-1","Hostname":"kubernetes-1","Address":"10.100.50.101"},"c":{"Name":"kubernetes-0","Hostname":"kubernetes-0","Address":"10.100.50.100"}}} maxMonId:2 outOfQuorum:]
2025-01-29 15:46:03.254176 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:46:03.254466 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:46:04.856294 I | op-mon: targeting the mon count 3
2025-01-29 15:46:04.879268 I | op-config: applying ceph settings:
[global]
mon allow pool delete   = true
mon cluster log file    = 
mon allow pool size one = true
2025-01-29 15:46:05.332101 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:46:05.332500 I | op-config: applying ceph settings:
[global]
log to file = true
2025-01-29 15:46:05.797744 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:46:05.797861 I | op-config: deleting "global" "log file" option from the mon configuration database
2025-01-29 15:46:06.030471 I | exec: exec timeout waiting for process rbd to return. Sending interrupt signal to the process
2025-01-29 15:46:06.047716 E | ceph-block-pool-controller: failed to reconcile CephBlockPool "rook-ceph/ceph-blockpool". failed to create pool "ceph-blockpool".: failed to configure pool "ceph-blockpool".: failed to initialize pool "ceph-blockpool" for RBD use. : signal: interrupt
2025-01-29 15:46:06.220008 I | op-config: successfully deleted "log file" option from the mon configuration database
2025-01-29 15:46:06.220041 I | op-mon: checking for basic quorum with existing mons
2025-01-29 15:46:06.220053 I | op-mon: setting mon "b" endpoints for hostnetwork mode
2025-01-29 15:46:06.220067 I | op-mon: setting mon "c" endpoints for hostnetwork mode
2025-01-29 15:46:06.220076 I | op-mon: setting mon "a" endpoints for hostnetwork mode
2025-01-29 15:46:06.240017 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.100.50.101:3300","10.100.50.100:3300","10.100.50.102:3300"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","radosNamespace":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":"","mirrorDaemonCount":0},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:b=10.100.50.101:3300,c=10.100.50.100:3300,a=10.100.50.102:3300 mapping:{"node":{"a":{"Name":"kubernetes-2","Hostname":"kubernetes-2","Address":"10.100.50.102"},"b":{"Name":"kubernetes-1","Hostname":"kubernetes-1","Address":"10.100.50.101"},"c":{"Name":"kubernetes-0","Hostname":"kubernetes-0","Address":"10.100.50.100"}}} maxMonId:2 outOfQuorum:]
2025-01-29 15:46:06.257595 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:46:06.257856 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:46:06.445319 I | clusterdisruption-controller: all "host" failure domains: []. osd is down in failure domain: "". active node drains: false. pg health: "cluster is not fully clean. PGs: [{StateName:unknown Count:1}]"
2025-01-29 15:46:06.450936 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2025-01-29 15:46:06.475405 I | op-mon: deployment for mon rook-ceph-mon-b already exists. updating if needed
2025-01-29 15:46:06.485671 I | op-k8sutil: deployment "rook-ceph-mon-b" did not change, nothing to update
2025-01-29 15:46:06.485700 I | op-mon: waiting for mon quorum with [b c a]
2025-01-29 15:46:07.048998 I | clusterdisruption-controller: all "host" failure domains: []. osd is down in failure domain: "". active node drains: false. pg health: "cluster is not fully clean. PGs: [{StateName:unknown Count:1}]"
2025-01-29 15:46:07.053764 I | ceph-spec: parsing mon endpoints: b=10.100.50.101:3300,c=10.100.50.100:3300,a=10.100.50.102:3300
2025-01-29 15:46:07.054963 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2025-01-29 15:46:07.464430 I | op-mon: mons running: [b c a]
2025-01-29 15:46:07.519573 I | ceph-block-pool-controller: creating pool "ceph-blockpool" in namespace "rook-ceph"
2025-01-29 15:46:08.319410 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:46:08.328227 I | op-mon: deployment for mon rook-ceph-mon-c already exists. updating if needed
2025-01-29 15:46:08.343339 I | op-k8sutil: deployment "rook-ceph-mon-c" did not change, nothing to update
2025-01-29 15:46:08.343371 I | op-mon: waiting for mon quorum with [b c a]
2025-01-29 15:46:08.387958 I | op-mon: mons running: [b c a]
2025-01-29 15:46:09.086148 I | cephclient: application "rbd" is already set on pool "ceph-blockpool"
2025-01-29 15:46:09.086196 I | cephclient: reconciling replicated pool ceph-blockpool succeeded
2025-01-29 15:46:09.086214 I | ceph-block-pool-controller: initializing pool "ceph-blockpool" for RBD use
2025-01-29 15:46:09.178032 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:46:09.187260 I | op-mon: deployment for mon rook-ceph-mon-a already exists. updating if needed
2025-01-29 15:46:09.202784 I | op-k8sutil: deployment "rook-ceph-mon-a" did not change, nothing to update
2025-01-29 15:46:09.202816 I | op-mon: waiting for mon quorum with [b c a]
2025-01-29 15:46:09.248803 I | op-mon: mons running: [b c a]
2025-01-29 15:46:09.838536 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:46:09.838560 I | op-mon: mons created: 3
2025-01-29 15:46:10.411885 I | op-mon: waiting for mon quorum with [b c a]
2025-01-29 15:46:10.457103 I | op-mon: mons running: [b c a]
2025-01-29 15:46:11.283104 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:46:11.283145 I | ceph-spec: ensuring cluster "rook-ceph" "public" network is configured to use CIDR(s) ""
2025-01-29 15:46:11.283156 I | ceph-spec: ensuring cluster "rook-ceph" "cluster" network is configured to use CIDR(s) ""
2025-01-29 15:46:11.283437 I | cephclient: getting or creating ceph auth key "client.csi-rbd-provisioner"
2025-01-29 15:46:11.822269 I | cephclient: getting or creating ceph auth key "client.csi-rbd-node"
2025-01-29 15:46:12.383069 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-provisioner"
2025-01-29 15:46:12.945206 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-node"
2025-01-29 15:46:13.739792 I | ceph-csi: created kubernetes csi secrets for cluster "rook-ceph"
2025-01-29 15:46:13.739825 I | cephclient: getting or creating ceph auth key "client.crash"
2025-01-29 15:46:14.282603 I | ceph-nodedaemon-controller: created kubernetes crash collector secret for cluster "rook-ceph"
2025-01-29 15:46:14.282627 I | cephclient: getting or creating ceph auth key "client.ceph-exporter"
2025-01-29 15:46:14.898108 I | ceph-nodedaemon-controller: created kubernetes exporter secret for cluster "rook-ceph"
2025-01-29 15:46:14.898143 I | op-config: deleting "global" "ms_service_mode" option from the mon configuration database
2025-01-29 15:46:15.412082 I | op-config: successfully deleted "ms_service_mode" option from the mon configuration database
2025-01-29 15:46:15.412118 I | op-config: deleting "global" "ms_client_mode" option from the mon configuration database
2025-01-29 15:46:15.880969 I | op-config: successfully deleted "ms_client_mode" option from the mon configuration database
2025-01-29 15:46:15.880993 I | op-config: deleting "global" "rbd_default_map_options" option from the mon configuration database
2025-01-29 15:46:16.313025 I | op-config: successfully deleted "rbd_default_map_options" option from the mon configuration database
2025-01-29 15:46:16.313051 I | op-config: deleting "global" "ms_cluster_mode" option from the mon configuration database
2025-01-29 15:46:16.725512 I | op-config: successfully deleted "ms_cluster_mode" option from the mon configuration database
2025-01-29 15:46:16.725874 I | op-config: applying ceph settings:
[global]
rbd_default_map_options = ms_mode=prefer-crc
2025-01-29 15:46:17.159063 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:46:17.159146 I | op-config: deleting "global" "ms_osd_compress_mode" option from the mon configuration database
2025-01-29 15:46:17.590912 I | op-config: successfully deleted "ms_osd_compress_mode" option from the mon configuration database
2025-01-29 15:46:17.590944 I | cephclient: create rbd-mirror bootstrap peer token "client.rbd-mirror-peer"
2025-01-29 15:46:17.590951 I | cephclient: getting or creating ceph auth key "client.rbd-mirror-peer"
2025-01-29 15:46:18.181264 I | cephclient: successfully created rbd-mirror bootstrap peer token for cluster "rook-ceph"
2025-01-29 15:46:18.228954 I | op-mgr: start running mgr
2025-01-29 15:46:18.239640 I | cephclient: getting or creating ceph auth key "mgr.a"
2025-01-29 15:46:18.749644 I | op-mgr: deployment for mgr rook-ceph-mgr-a already exists. updating if needed
2025-01-29 15:46:18.764236 I | op-k8sutil: deployment "rook-ceph-mgr-a" did not change, nothing to update
2025-01-29 15:46:18.764266 I | cephclient: getting or creating ceph auth key "mgr.b"
2025-01-29 15:46:19.398255 I | op-mgr: deployment for mgr rook-ceph-mgr-b already exists. updating if needed
2025-01-29 15:46:19.413349 I | op-k8sutil: deployment "rook-ceph-mgr-b" did not change, nothing to update
2025-01-29 15:46:19.522616 E | ceph-cluster-controller: failed to reconcile CephCluster "rook-ceph/rook-ceph". failed to reconcile cluster "rook-ceph": failed to configure local ceph cluster: failed to create cluster: failed to start ceph mgr: failed to enable mgr services: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-mgr" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:46:20.163751 I | ceph-cluster-controller: reconciling ceph cluster in namespace "rook-ceph"
2025-01-29 15:46:20.173346 I | ceph-spec: parsing mon endpoints: b=10.100.50.101:3300,c=10.100.50.100:3300,a=10.100.50.102:3300
2025-01-29 15:46:20.220495 I | ceph-spec: detecting the ceph image version for image quay.io/ceph/ceph:v19.2.0...
2025-01-29 15:46:22.074893 I | ceph-spec: detected ceph image version: "19.2.0-0 squid"
2025-01-29 15:46:22.074922 I | ceph-cluster-controller: validating ceph version from provided image
2025-01-29 15:46:22.085045 I | ceph-spec: parsing mon endpoints: b=10.100.50.101:3300,c=10.100.50.100:3300,a=10.100.50.102:3300
2025-01-29 15:46:22.089523 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:46:22.089840 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:46:22.718447 I | ceph-cluster-controller: cluster "rook-ceph": version "19.2.0-0 squid" detected for image "quay.io/ceph/ceph:v19.2.0"
2025-01-29 15:46:22.801196 I | op-mon: start running mons
2025-01-29 15:46:22.806161 I | ceph-spec: parsing mon endpoints: b=10.100.50.101:3300,c=10.100.50.100:3300,a=10.100.50.102:3300
2025-01-29 15:46:22.822212 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.100.50.101:3300","10.100.50.100:3300","10.100.50.102:3300"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","radosNamespace":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":"","mirrorDaemonCount":0},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:c=10.100.50.100:3300,a=10.100.50.102:3300,b=10.100.50.101:3300 mapping:{"node":{"a":{"Name":"kubernetes-2","Hostname":"kubernetes-2","Address":"10.100.50.102"},"b":{"Name":"kubernetes-1","Hostname":"kubernetes-1","Address":"10.100.50.101"},"c":{"Name":"kubernetes-0","Hostname":"kubernetes-0","Address":"10.100.50.100"}}} maxMonId:2 outOfQuorum:]
2025-01-29 15:46:23.369422 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:46:23.369738 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:46:24.088085 I | exec: exec timeout waiting for process rbd to return. Sending interrupt signal to the process
2025-01-29 15:46:24.103821 E | ceph-block-pool-controller: failed to reconcile CephBlockPool "rook-ceph/ceph-blockpool". failed to create pool "ceph-blockpool".: failed to configure pool "ceph-blockpool".: failed to initialize pool "ceph-blockpool" for RBD use. : signal: interrupt
2025-01-29 15:46:25.172405 I | op-mon: targeting the mon count 3
2025-01-29 15:46:25.194762 I | op-config: applying ceph settings:
[global]
mon allow pool delete   = true
mon cluster log file    = 
mon allow pool size one = true
2025-01-29 15:46:25.369956 I | ceph-spec: parsing mon endpoints: c=10.100.50.100:3300,a=10.100.50.102:3300,b=10.100.50.101:3300
2025-01-29 15:46:25.608272 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:46:25.608644 I | op-config: applying ceph settings:
[global]
log to file = true
2025-01-29 15:46:25.829772 I | ceph-block-pool-controller: creating pool "ceph-blockpool" in namespace "rook-ceph"
2025-01-29 15:46:25.974220 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:46:26.002203 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:46:26.002294 I | op-config: deleting "global" "log file" option from the mon configuration database
2025-01-29 15:46:26.408115 I | op-config: successfully deleted "log file" option from the mon configuration database
2025-01-29 15:46:26.408155 I | op-mon: checking for basic quorum with existing mons
2025-01-29 15:46:26.408164 I | op-mon: setting mon "b" endpoints for hostnetwork mode
2025-01-29 15:46:26.408173 I | op-mon: setting mon "c" endpoints for hostnetwork mode
2025-01-29 15:46:26.408178 I | op-mon: setting mon "a" endpoints for hostnetwork mode
2025-01-29 15:46:26.573301 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.100.50.101:3300","10.100.50.100:3300","10.100.50.102:3300"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","radosNamespace":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":"","mirrorDaemonCount":0},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:b=10.100.50.101:3300,c=10.100.50.100:3300,a=10.100.50.102:3300 mapping:{"node":{"a":{"Name":"kubernetes-2","Hostname":"kubernetes-2","Address":"10.100.50.102"},"b":{"Name":"kubernetes-1","Hostname":"kubernetes-1","Address":"10.100.50.101"},"c":{"Name":"kubernetes-0","Hostname":"kubernetes-0","Address":"10.100.50.100"}}} maxMonId:2 outOfQuorum:]
2025-01-29 15:46:27.170042 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:46:27.170330 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:46:27.302410 I | cephclient: application "rbd" is already set on pool "ceph-blockpool"
2025-01-29 15:46:27.302435 I | cephclient: reconciling replicated pool ceph-blockpool succeeded
2025-01-29 15:46:27.302444 I | ceph-block-pool-controller: initializing pool "ceph-blockpool" for RBD use
2025-01-29 15:46:27.791616 I | op-mon: deployment for mon rook-ceph-mon-b already exists. updating if needed
2025-01-29 15:46:27.806008 I | op-k8sutil: deployment "rook-ceph-mon-b" did not change, nothing to update
2025-01-29 15:46:27.806045 I | op-mon: waiting for mon quorum with [b c a]
2025-01-29 15:46:28.374653 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:46:28.780473 I | op-mon: mons running: [b c a]
2025-01-29 15:46:29.321406 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:46:29.331034 I | op-mon: deployment for mon rook-ceph-mon-c already exists. updating if needed
2025-01-29 15:46:29.346087 I | op-k8sutil: deployment "rook-ceph-mon-c" did not change, nothing to update
2025-01-29 15:46:29.346127 I | op-mon: waiting for mon quorum with [b c a]
2025-01-29 15:46:29.389411 I | op-mon: mons running: [b c a]
2025-01-29 15:46:30.100032 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:46:30.109242 I | op-mon: deployment for mon rook-ceph-mon-a already exists. updating if needed
2025-01-29 15:46:30.124192 I | op-k8sutil: deployment "rook-ceph-mon-a" did not change, nothing to update
2025-01-29 15:46:30.124228 I | op-mon: waiting for mon quorum with [b c a]
2025-01-29 15:46:30.168992 I | op-mon: mons running: [b c a]
2025-01-29 15:46:30.788449 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:46:30.788483 I | op-mon: mons created: 3
2025-01-29 15:46:31.394385 I | op-mon: waiting for mon quorum with [b c a]
2025-01-29 15:46:31.439028 I | op-mon: mons running: [b c a]
2025-01-29 15:46:32.053860 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:46:32.053898 I | ceph-spec: ensuring cluster "rook-ceph" "public" network is configured to use CIDR(s) ""
2025-01-29 15:46:32.053906 I | ceph-spec: ensuring cluster "rook-ceph" "cluster" network is configured to use CIDR(s) ""
2025-01-29 15:46:32.054139 I | cephclient: getting or creating ceph auth key "client.csi-rbd-provisioner"
2025-01-29 15:46:32.615985 I | cephclient: getting or creating ceph auth key "client.csi-rbd-node"
2025-01-29 15:46:33.278816 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-provisioner"
2025-01-29 15:46:33.839069 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-node"
2025-01-29 15:46:34.446544 I | ceph-csi: created kubernetes csi secrets for cluster "rook-ceph"
2025-01-29 15:46:34.446574 I | cephclient: getting or creating ceph auth key "client.crash"
2025-01-29 15:46:34.613931 E | ceph-nodedaemon-controller: node reconcile failed: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-exporter" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:46:35.270674 I | ceph-nodedaemon-controller: created kubernetes crash collector secret for cluster "rook-ceph"
2025-01-29 15:46:35.270708 I | cephclient: getting or creating ceph auth key "client.ceph-exporter"
2025-01-29 15:46:35.885119 I | ceph-nodedaemon-controller: created kubernetes exporter secret for cluster "rook-ceph"
2025-01-29 15:46:35.885150 I | op-config: deleting "global" "ms_service_mode" option from the mon configuration database
2025-01-29 15:46:36.352505 I | op-config: successfully deleted "ms_service_mode" option from the mon configuration database
2025-01-29 15:46:36.352541 I | op-config: deleting "global" "ms_client_mode" option from the mon configuration database
2025-01-29 15:46:36.800200 I | op-config: successfully deleted "ms_client_mode" option from the mon configuration database
2025-01-29 15:46:36.800236 I | op-config: deleting "global" "rbd_default_map_options" option from the mon configuration database
2025-01-29 15:46:37.005508 I | clusterdisruption-controller: all "host" failure domains: []. osd is down in failure domain: "". active node drains: false. pg health: "cluster is not fully clean. PGs: [{StateName:unknown Count:1}]"
2025-01-29 15:46:37.010594 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2025-01-29 15:46:37.235368 I | op-config: successfully deleted "rbd_default_map_options" option from the mon configuration database
2025-01-29 15:46:37.235394 I | op-config: deleting "global" "ms_cluster_mode" option from the mon configuration database
2025-01-29 15:46:37.696057 I | op-config: successfully deleted "ms_cluster_mode" option from the mon configuration database
2025-01-29 15:46:37.697359 I | op-config: applying ceph settings:
[global]
rbd_default_map_options = ms_mode=prefer-crc
2025-01-29 15:46:37.876393 I | clusterdisruption-controller: all "host" failure domains: []. osd is down in failure domain: "". active node drains: false. pg health: "cluster is not fully clean. PGs: [{StateName:unknown Count:1}]"
2025-01-29 15:46:37.882388 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2025-01-29 15:46:38.137176 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:46:38.137303 I | op-config: deleting "global" "ms_osd_compress_mode" option from the mon configuration database
2025-01-29 15:46:38.805797 I | op-config: successfully deleted "ms_osd_compress_mode" option from the mon configuration database
2025-01-29 15:46:38.805853 I | cephclient: create rbd-mirror bootstrap peer token "client.rbd-mirror-peer"
2025-01-29 15:46:38.805864 I | cephclient: getting or creating ceph auth key "client.rbd-mirror-peer"
2025-01-29 15:46:39.372266 I | cephclient: successfully created rbd-mirror bootstrap peer token for cluster "rook-ceph"
2025-01-29 15:46:39.414165 I | op-mgr: start running mgr
2025-01-29 15:46:39.425186 I | cephclient: getting or creating ceph auth key "mgr.a"
2025-01-29 15:46:40.032470 I | op-mgr: deployment for mgr rook-ceph-mgr-a already exists. updating if needed
2025-01-29 15:46:40.047401 I | op-k8sutil: deployment "rook-ceph-mgr-a" did not change, nothing to update
2025-01-29 15:46:40.047440 I | cephclient: getting or creating ceph auth key "mgr.b"
2025-01-29 15:46:40.622217 I | op-mgr: deployment for mgr rook-ceph-mgr-b already exists. updating if needed
2025-01-29 15:46:40.637112 I | op-k8sutil: deployment "rook-ceph-mgr-b" did not change, nothing to update
2025-01-29 15:46:40.739745 E | ceph-cluster-controller: failed to reconcile CephCluster "rook-ceph/rook-ceph". failed to reconcile cluster "rook-ceph": failed to configure local ceph cluster: failed to create cluster: failed to start ceph mgr: failed to enable mgr services: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-mgr" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:46:42.020929 I | ceph-cluster-controller: reconciling ceph cluster in namespace "rook-ceph"
2025-01-29 15:46:42.030720 I | ceph-spec: parsing mon endpoints: b=10.100.50.101:3300,c=10.100.50.100:3300,a=10.100.50.102:3300
2025-01-29 15:46:42.076587 I | ceph-spec: detecting the ceph image version for image quay.io/ceph/ceph:v19.2.0...
2025-01-29 15:46:42.303218 I | exec: exec timeout waiting for process rbd to return. Sending interrupt signal to the process
2025-01-29 15:46:42.319530 E | ceph-block-pool-controller: failed to reconcile CephBlockPool "rook-ceph/ceph-blockpool". failed to create pool "ceph-blockpool".: failed to configure pool "ceph-blockpool".: failed to initialize pool "ceph-blockpool" for RBD use. : signal: interrupt
2025-01-29 15:46:43.610173 I | ceph-spec: parsing mon endpoints: b=10.100.50.101:3300,c=10.100.50.100:3300,a=10.100.50.102:3300
2025-01-29 15:46:44.084197 I | ceph-block-pool-controller: creating pool "ceph-blockpool" in namespace "rook-ceph"
2025-01-29 15:46:44.234381 I | ceph-spec: detected ceph image version: "19.2.0-0 squid"
2025-01-29 15:46:44.234410 I | ceph-cluster-controller: validating ceph version from provided image
2025-01-29 15:46:44.242692 I | ceph-spec: parsing mon endpoints: b=10.100.50.101:3300,c=10.100.50.100:3300,a=10.100.50.102:3300
2025-01-29 15:46:44.247596 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:46:44.247837 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:46:44.817572 I | ceph-cluster-controller: cluster "rook-ceph": version "19.2.0-0 squid" detected for image "quay.io/ceph/ceph:v19.2.0"
2025-01-29 15:46:44.915717 I | op-mon: start running mons
2025-01-29 15:46:44.924191 I | ceph-spec: parsing mon endpoints: b=10.100.50.101:3300,c=10.100.50.100:3300,a=10.100.50.102:3300
2025-01-29 15:46:44.954065 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.100.50.101:3300","10.100.50.100:3300","10.100.50.102:3300"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","radosNamespace":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":"","mirrorDaemonCount":0},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:c=10.100.50.100:3300,a=10.100.50.102:3300,b=10.100.50.101:3300 mapping:{"node":{"a":{"Name":"kubernetes-2","Hostname":"kubernetes-2","Address":"10.100.50.102"},"b":{"Name":"kubernetes-1","Hostname":"kubernetes-1","Address":"10.100.50.101"},"c":{"Name":"kubernetes-0","Hostname":"kubernetes-0","Address":"10.100.50.100"}}} maxMonId:2 outOfQuorum:]
2025-01-29 15:46:45.418215 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:46:45.418551 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:46:45.740269 I | cephclient: application "rbd" is already set on pool "ceph-blockpool"
2025-01-29 15:46:45.740309 I | cephclient: reconciling replicated pool ceph-blockpool succeeded
2025-01-29 15:46:45.740319 I | ceph-block-pool-controller: initializing pool "ceph-blockpool" for RBD use
2025-01-29 15:46:47.018871 I | op-mon: targeting the mon count 3
2025-01-29 15:46:47.039848 I | op-config: applying ceph settings:
[global]
mon allow pool delete   = true
mon cluster log file    = 
mon allow pool size one = true
2025-01-29 15:46:47.530932 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:46:47.531284 I | op-config: applying ceph settings:
[global]
log to file = true
2025-01-29 15:46:48.236228 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:46:48.236359 I | op-config: deleting "global" "log file" option from the mon configuration database
2025-01-29 15:46:48.766475 I | op-config: successfully deleted "log file" option from the mon configuration database
2025-01-29 15:46:48.766509 I | op-mon: checking for basic quorum with existing mons
2025-01-29 15:46:48.766521 I | op-mon: setting mon "c" endpoints for hostnetwork mode
2025-01-29 15:46:48.766535 I | op-mon: setting mon "a" endpoints for hostnetwork mode
2025-01-29 15:46:48.766543 I | op-mon: setting mon "b" endpoints for hostnetwork mode
2025-01-29 15:46:48.790684 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.100.50.101:3300","10.100.50.100:3300","10.100.50.102:3300"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","radosNamespace":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":"","mirrorDaemonCount":0},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:b=10.100.50.101:3300,c=10.100.50.100:3300,a=10.100.50.102:3300 mapping:{"node":{"a":{"Name":"kubernetes-2","Hostname":"kubernetes-2","Address":"10.100.50.102"},"b":{"Name":"kubernetes-1","Hostname":"kubernetes-1","Address":"10.100.50.101"},"c":{"Name":"kubernetes-0","Hostname":"kubernetes-0","Address":"10.100.50.100"}}} maxMonId:2 outOfQuorum:]
2025-01-29 15:46:48.805347 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:46:48.805596 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:46:48.834692 I | op-mon: deployment for mon rook-ceph-mon-c already exists. updating if needed
2025-01-29 15:46:48.845424 I | op-k8sutil: deployment "rook-ceph-mon-c" did not change, nothing to update
2025-01-29 15:46:48.845444 I | op-mon: waiting for mon quorum with [c a b]
2025-01-29 15:46:49.228520 I | op-mon: mons running: [c a b]
2025-01-29 15:46:49.774016 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:46:49.782961 I | op-mon: deployment for mon rook-ceph-mon-a already exists. updating if needed
2025-01-29 15:46:49.797251 I | op-k8sutil: deployment "rook-ceph-mon-a" did not change, nothing to update
2025-01-29 15:46:49.797279 I | op-mon: waiting for mon quorum with [c a b]
2025-01-29 15:46:49.840372 I | op-mon: mons running: [c a b]
2025-01-29 15:46:50.399944 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:46:50.408157 I | op-mon: deployment for mon rook-ceph-mon-b already exists. updating if needed
2025-01-29 15:46:50.423287 I | op-k8sutil: deployment "rook-ceph-mon-b" did not change, nothing to update
2025-01-29 15:46:50.423317 I | op-mon: waiting for mon quorum with [c a b]
2025-01-29 15:46:50.468203 I | op-mon: mons running: [c a b]
2025-01-29 15:46:51.266424 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:46:51.266456 I | op-mon: mons created: 3
2025-01-29 15:46:51.799333 I | op-mon: waiting for mon quorum with [c a b]
2025-01-29 15:46:51.842723 I | op-mon: mons running: [c a b]
2025-01-29 15:46:52.428433 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:46:52.428470 I | ceph-spec: ensuring cluster "rook-ceph" "public" network is configured to use CIDR(s) ""
2025-01-29 15:46:52.428481 I | ceph-spec: ensuring cluster "rook-ceph" "cluster" network is configured to use CIDR(s) ""
2025-01-29 15:46:52.428757 I | cephclient: getting or creating ceph auth key "client.csi-rbd-provisioner"
2025-01-29 15:46:53.283311 I | cephclient: getting or creating ceph auth key "client.csi-rbd-node"
2025-01-29 15:46:53.849716 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-provisioner"
2025-01-29 15:46:54.447392 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-node"
2025-01-29 15:46:55.224575 I | ceph-csi: created kubernetes csi secrets for cluster "rook-ceph"
2025-01-29 15:46:55.224604 I | cephclient: getting or creating ceph auth key "client.crash"
2025-01-29 15:46:55.781159 I | ceph-nodedaemon-controller: created kubernetes crash collector secret for cluster "rook-ceph"
2025-01-29 15:46:55.781205 I | cephclient: getting or creating ceph auth key "client.ceph-exporter"
2025-01-29 15:46:56.381844 I | ceph-nodedaemon-controller: created kubernetes exporter secret for cluster "rook-ceph"
2025-01-29 15:46:56.381883 I | op-config: deleting "global" "ms_cluster_mode" option from the mon configuration database
2025-01-29 15:46:56.877972 I | op-config: successfully deleted "ms_cluster_mode" option from the mon configuration database
2025-01-29 15:46:56.878010 I | op-config: deleting "global" "ms_service_mode" option from the mon configuration database
2025-01-29 15:46:57.424250 I | op-config: successfully deleted "ms_service_mode" option from the mon configuration database
2025-01-29 15:46:57.424281 I | op-config: deleting "global" "ms_client_mode" option from the mon configuration database
2025-01-29 15:46:57.910564 I | op-config: successfully deleted "ms_client_mode" option from the mon configuration database
2025-01-29 15:46:57.910597 I | op-config: deleting "global" "rbd_default_map_options" option from the mon configuration database
2025-01-29 15:46:58.353916 I | op-config: successfully deleted "rbd_default_map_options" option from the mon configuration database
2025-01-29 15:46:58.354243 I | op-config: applying ceph settings:
[global]
rbd_default_map_options = ms_mode=prefer-crc
2025-01-29 15:46:58.814725 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:46:58.814845 I | op-config: deleting "global" "ms_osd_compress_mode" option from the mon configuration database
2025-01-29 15:46:59.290223 I | op-config: successfully deleted "ms_osd_compress_mode" option from the mon configuration database
2025-01-29 15:46:59.290259 I | cephclient: create rbd-mirror bootstrap peer token "client.rbd-mirror-peer"
2025-01-29 15:46:59.290316 I | cephclient: getting or creating ceph auth key "client.rbd-mirror-peer"
2025-01-29 15:46:59.822302 I | cephclient: successfully created rbd-mirror bootstrap peer token for cluster "rook-ceph"
2025-01-29 15:46:59.864586 I | op-mgr: start running mgr
2025-01-29 15:46:59.877575 I | cephclient: getting or creating ceph auth key "mgr.a"
2025-01-29 15:47:00.548806 I | op-mgr: deployment for mgr rook-ceph-mgr-a already exists. updating if needed
2025-01-29 15:47:00.563690 I | op-k8sutil: deployment "rook-ceph-mgr-a" did not change, nothing to update
2025-01-29 15:47:00.563723 I | cephclient: getting or creating ceph auth key "mgr.b"
2025-01-29 15:47:00.741276 I | exec: exec timeout waiting for process rbd to return. Sending interrupt signal to the process
2025-01-29 15:47:00.757492 E | ceph-block-pool-controller: failed to reconcile CephBlockPool "rook-ceph/ceph-blockpool". failed to create pool "ceph-blockpool".: failed to configure pool "ceph-blockpool".: failed to initialize pool "ceph-blockpool" for RBD use. : signal: interrupt
2025-01-29 15:47:01.140155 I | op-mgr: deployment for mgr rook-ceph-mgr-b already exists. updating if needed
2025-01-29 15:47:01.154325 I | op-k8sutil: deployment "rook-ceph-mgr-b" did not change, nothing to update
2025-01-29 15:47:01.259022 E | ceph-cluster-controller: failed to reconcile CephCluster "rook-ceph/rook-ceph". failed to reconcile cluster "rook-ceph": failed to configure local ceph cluster: failed to create cluster: failed to start ceph mgr: failed to enable mgr services: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-mgr" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:47:03.326775 I | ceph-spec: parsing mon endpoints: b=10.100.50.101:3300,c=10.100.50.100:3300,a=10.100.50.102:3300
2025-01-29 15:47:03.753124 I | ceph-block-pool-controller: creating pool "ceph-blockpool" in namespace "rook-ceph"
2025-01-29 15:47:03.819888 I | ceph-cluster-controller: reconciling ceph cluster in namespace "rook-ceph"
2025-01-29 15:47:03.828940 I | ceph-spec: parsing mon endpoints: b=10.100.50.101:3300,c=10.100.50.100:3300,a=10.100.50.102:3300
2025-01-29 15:47:03.874604 I | ceph-spec: detecting the ceph image version for image quay.io/ceph/ceph:v19.2.0...
2025-01-29 15:47:05.246875 I | cephclient: application "rbd" is already set on pool "ceph-blockpool"
2025-01-29 15:47:05.246908 I | cephclient: reconciling replicated pool ceph-blockpool succeeded
2025-01-29 15:47:05.246919 I | ceph-block-pool-controller: initializing pool "ceph-blockpool" for RBD use
2025-01-29 15:47:06.345349 I | ceph-spec: detected ceph image version: "19.2.0-0 squid"
2025-01-29 15:47:06.345378 I | ceph-cluster-controller: validating ceph version from provided image
2025-01-29 15:47:06.355153 I | ceph-spec: parsing mon endpoints: b=10.100.50.101:3300,c=10.100.50.100:3300,a=10.100.50.102:3300
2025-01-29 15:47:06.359896 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:47:06.360148 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:47:06.907937 I | ceph-cluster-controller: cluster "rook-ceph": version "19.2.0-0 squid" detected for image "quay.io/ceph/ceph:v19.2.0"
2025-01-29 15:47:06.961719 I | op-mon: start running mons
2025-01-29 15:47:06.966666 I | ceph-spec: parsing mon endpoints: b=10.100.50.101:3300,c=10.100.50.100:3300,a=10.100.50.102:3300
2025-01-29 15:47:06.982900 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.100.50.101:3300","10.100.50.100:3300","10.100.50.102:3300"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","radosNamespace":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":"","mirrorDaemonCount":0},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300 mapping:{"node":{"a":{"Name":"kubernetes-2","Hostname":"kubernetes-2","Address":"10.100.50.102"},"b":{"Name":"kubernetes-1","Hostname":"kubernetes-1","Address":"10.100.50.101"},"c":{"Name":"kubernetes-0","Hostname":"kubernetes-0","Address":"10.100.50.100"}}} maxMonId:2 outOfQuorum:]
2025-01-29 15:47:07.528833 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:47:07.529134 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:47:07.553170 I | clusterdisruption-controller: all "host" failure domains: []. osd is down in failure domain: "". active node drains: false. pg health: "cluster is not fully clean. PGs: [{StateName:unknown Count:1}]"
2025-01-29 15:47:07.558822 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2025-01-29 15:47:08.801954 I | clusterdisruption-controller: all "host" failure domains: []. osd is down in failure domain: "". active node drains: false. pg health: "cluster is not fully clean. PGs: [{StateName:unknown Count:1}]"
2025-01-29 15:47:08.807683 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2025-01-29 15:47:09.130259 I | op-mon: targeting the mon count 3
2025-01-29 15:47:09.151643 I | op-config: applying ceph settings:
[global]
mon allow pool delete   = true
mon cluster log file    = 
mon allow pool size one = true
2025-01-29 15:47:09.596330 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:47:09.596721 I | op-config: applying ceph settings:
[global]
log to file = true
2025-01-29 15:47:10.028810 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:47:10.028938 I | op-config: deleting "global" "log file" option from the mon configuration database
2025-01-29 15:47:10.733637 I | op-config: successfully deleted "log file" option from the mon configuration database
2025-01-29 15:47:10.733667 I | op-mon: checking for basic quorum with existing mons
2025-01-29 15:47:10.733680 I | op-mon: setting mon "a" endpoints for hostnetwork mode
2025-01-29 15:47:10.733693 I | op-mon: setting mon "b" endpoints for hostnetwork mode
2025-01-29 15:47:10.733705 I | op-mon: setting mon "c" endpoints for hostnetwork mode
2025-01-29 15:47:10.754609 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.100.50.101:3300","10.100.50.100:3300","10.100.50.102:3300"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","radosNamespace":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":"","mirrorDaemonCount":0},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:b=10.100.50.101:3300,c=10.100.50.100:3300,a=10.100.50.102:3300 mapping:{"node":{"a":{"Name":"kubernetes-2","Hostname":"kubernetes-2","Address":"10.100.50.102"},"b":{"Name":"kubernetes-1","Hostname":"kubernetes-1","Address":"10.100.50.101"},"c":{"Name":"kubernetes-0","Hostname":"kubernetes-0","Address":"10.100.50.100"}}} maxMonId:2 outOfQuorum:]
2025-01-29 15:47:10.769980 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:47:10.770246 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:47:10.803986 I | op-mon: deployment for mon rook-ceph-mon-a already exists. updating if needed
2025-01-29 15:47:10.819139 I | op-k8sutil: deployment "rook-ceph-mon-a" did not change, nothing to update
2025-01-29 15:47:10.819169 I | op-mon: waiting for mon quorum with [a b c]
2025-01-29 15:47:11.348631 I | op-mon: mons running: [a b c]
2025-01-29 15:47:11.917061 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:47:11.925552 I | op-mon: deployment for mon rook-ceph-mon-b already exists. updating if needed
2025-01-29 15:47:11.939886 I | op-k8sutil: deployment "rook-ceph-mon-b" did not change, nothing to update
2025-01-29 15:47:11.939926 I | op-mon: waiting for mon quorum with [a b c]
2025-01-29 15:47:11.983256 I | op-mon: mons running: [a b c]
2025-01-29 15:47:12.809035 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:47:12.817124 I | op-mon: deployment for mon rook-ceph-mon-c already exists. updating if needed
2025-01-29 15:47:12.829171 I | op-k8sutil: deployment "rook-ceph-mon-c" did not change, nothing to update
2025-01-29 15:47:12.829194 I | op-mon: waiting for mon quorum with [a b c]
2025-01-29 15:47:12.875292 I | op-mon: mons running: [a b c]
2025-01-29 15:47:13.645271 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:47:13.645306 I | op-mon: mons created: 3
2025-01-29 15:47:14.197510 I | op-mon: waiting for mon quorum with [a b c]
2025-01-29 15:47:14.243998 I | op-mon: mons running: [a b c]
2025-01-29 15:47:14.739640 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:47:14.739684 I | ceph-spec: ensuring cluster "rook-ceph" "public" network is configured to use CIDR(s) ""
2025-01-29 15:47:14.739708 I | ceph-spec: ensuring cluster "rook-ceph" "cluster" network is configured to use CIDR(s) ""
2025-01-29 15:47:14.739942 I | cephclient: getting or creating ceph auth key "client.csi-rbd-provisioner"
2025-01-29 15:47:15.290723 I | cephclient: getting or creating ceph auth key "client.csi-rbd-node"
2025-01-29 15:47:15.828080 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-provisioner"
2025-01-29 15:47:16.426439 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-node"
2025-01-29 15:47:17.278389 I | ceph-csi: created kubernetes csi secrets for cluster "rook-ceph"
2025-01-29 15:47:17.278423 I | cephclient: getting or creating ceph auth key "client.crash"
2025-01-29 15:47:17.854607 I | ceph-nodedaemon-controller: created kubernetes crash collector secret for cluster "rook-ceph"
2025-01-29 15:47:17.854646 I | cephclient: getting or creating ceph auth key "client.ceph-exporter"
2025-01-29 15:47:18.488010 I | ceph-nodedaemon-controller: created kubernetes exporter secret for cluster "rook-ceph"
2025-01-29 15:47:18.488048 I | op-config: deleting "global" "ms_service_mode" option from the mon configuration database
2025-01-29 15:47:19.190913 I | op-config: successfully deleted "ms_service_mode" option from the mon configuration database
2025-01-29 15:47:19.190951 I | op-config: deleting "global" "ms_client_mode" option from the mon configuration database
2025-01-29 15:47:19.633746 I | op-config: successfully deleted "ms_client_mode" option from the mon configuration database
2025-01-29 15:47:19.633783 I | op-config: deleting "global" "rbd_default_map_options" option from the mon configuration database
2025-01-29 15:47:20.117656 I | op-config: successfully deleted "rbd_default_map_options" option from the mon configuration database
2025-01-29 15:47:20.117692 I | op-config: deleting "global" "ms_cluster_mode" option from the mon configuration database
2025-01-29 15:47:20.248098 I | exec: exec timeout waiting for process rbd to return. Sending interrupt signal to the process
2025-01-29 15:47:20.264768 E | ceph-block-pool-controller: failed to reconcile CephBlockPool "rook-ceph/ceph-blockpool". failed to create pool "ceph-blockpool".: failed to configure pool "ceph-blockpool".: failed to initialize pool "ceph-blockpool" for RBD use. : signal: interrupt
2025-01-29 15:47:20.557559 I | op-config: successfully deleted "ms_cluster_mode" option from the mon configuration database
2025-01-29 15:47:20.557954 I | op-config: applying ceph settings:
[global]
rbd_default_map_options = ms_mode=prefer-crc
2025-01-29 15:47:21.083595 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:47:21.083725 I | op-config: deleting "global" "ms_osd_compress_mode" option from the mon configuration database
2025-01-29 15:47:21.769486 I | op-config: successfully deleted "ms_osd_compress_mode" option from the mon configuration database
2025-01-29 15:47:21.769530 I | cephclient: create rbd-mirror bootstrap peer token "client.rbd-mirror-peer"
2025-01-29 15:47:21.769539 I | cephclient: getting or creating ceph auth key "client.rbd-mirror-peer"
2025-01-29 15:47:22.324207 I | cephclient: successfully created rbd-mirror bootstrap peer token for cluster "rook-ceph"
2025-01-29 15:47:22.374326 I | op-mgr: start running mgr
2025-01-29 15:47:22.385799 I | cephclient: getting or creating ceph auth key "mgr.a"
2025-01-29 15:47:23.267288 I | op-mgr: deployment for mgr rook-ceph-mgr-a already exists. updating if needed
2025-01-29 15:47:23.281808 I | op-k8sutil: deployment "rook-ceph-mgr-a" did not change, nothing to update
2025-01-29 15:47:23.281841 I | cephclient: getting or creating ceph auth key "mgr.b"
2025-01-29 15:47:23.937305 I | op-mgr: deployment for mgr rook-ceph-mgr-b already exists. updating if needed
2025-01-29 15:47:23.954461 I | op-k8sutil: deployment "rook-ceph-mgr-b" did not change, nothing to update
2025-01-29 15:47:24.020181 E | ceph-cluster-controller: failed to reconcile CephCluster "rook-ceph/rook-ceph". failed to reconcile cluster "rook-ceph": failed to configure local ceph cluster: failed to create cluster: failed to start ceph mgr: failed to enable mgr services: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-mgr" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:47:25.395138 I | ceph-spec: parsing mon endpoints: b=10.100.50.101:3300,c=10.100.50.100:3300,a=10.100.50.102:3300
2025-01-29 15:47:25.855740 I | ceph-block-pool-controller: creating pool "ceph-blockpool" in namespace "rook-ceph"
2025-01-29 15:47:27.261546 I | cephclient: application "rbd" is already set on pool "ceph-blockpool"
2025-01-29 15:47:27.261581 I | cephclient: reconciling replicated pool ceph-blockpool succeeded
2025-01-29 15:47:27.261591 I | ceph-block-pool-controller: initializing pool "ceph-blockpool" for RBD use
2025-01-29 15:47:29.141357 I | ceph-cluster-controller: reconciling ceph cluster in namespace "rook-ceph"
2025-01-29 15:47:29.153078 I | ceph-spec: parsing mon endpoints: b=10.100.50.101:3300,c=10.100.50.100:3300,a=10.100.50.102:3300
2025-01-29 15:47:29.200509 I | ceph-spec: detecting the ceph image version for image quay.io/ceph/ceph:v19.2.0...
2025-01-29 15:47:31.469829 I | ceph-spec: detected ceph image version: "19.2.0-0 squid"
2025-01-29 15:47:31.469859 I | ceph-cluster-controller: validating ceph version from provided image
2025-01-29 15:47:31.478580 I | ceph-spec: parsing mon endpoints: b=10.100.50.101:3300,c=10.100.50.100:3300,a=10.100.50.102:3300
2025-01-29 15:47:31.483220 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:47:31.483507 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:47:32.104629 I | ceph-cluster-controller: cluster "rook-ceph": version "19.2.0-0 squid" detected for image "quay.io/ceph/ceph:v19.2.0"
2025-01-29 15:47:32.201845 I | op-mon: start running mons
2025-01-29 15:47:32.210813 I | ceph-spec: parsing mon endpoints: b=10.100.50.101:3300,c=10.100.50.100:3300,a=10.100.50.102:3300
2025-01-29 15:47:32.242455 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.100.50.101:3300","10.100.50.100:3300","10.100.50.102:3300"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","radosNamespace":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":"","mirrorDaemonCount":0},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:b=10.100.50.101:3300,c=10.100.50.100:3300,a=10.100.50.102:3300 mapping:{"node":{"a":{"Name":"kubernetes-2","Hostname":"kubernetes-2","Address":"10.100.50.102"},"b":{"Name":"kubernetes-1","Hostname":"kubernetes-1","Address":"10.100.50.101"},"c":{"Name":"kubernetes-0","Hostname":"kubernetes-0","Address":"10.100.50.100"}}} maxMonId:2 outOfQuorum:]
2025-01-29 15:47:32.654961 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:47:32.655341 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:47:34.256605 I | op-mon: targeting the mon count 3
2025-01-29 15:47:34.277951 I | op-config: applying ceph settings:
[global]
mon allow pool delete   = true
mon cluster log file    = 
mon allow pool size one = true
2025-01-29 15:47:34.712824 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:47:34.713240 I | op-config: applying ceph settings:
[global]
log to file = true
2025-01-29 15:47:35.136288 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:47:35.136397 I | op-config: deleting "global" "log file" option from the mon configuration database
2025-01-29 15:47:35.586504 I | op-config: successfully deleted "log file" option from the mon configuration database
2025-01-29 15:47:35.586530 I | op-mon: checking for basic quorum with existing mons
2025-01-29 15:47:35.586538 I | op-mon: setting mon "b" endpoints for hostnetwork mode
2025-01-29 15:47:35.586548 I | op-mon: setting mon "c" endpoints for hostnetwork mode
2025-01-29 15:47:35.586553 I | op-mon: setting mon "a" endpoints for hostnetwork mode
2025-01-29 15:47:35.606346 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.100.50.101:3300","10.100.50.100:3300","10.100.50.102:3300"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","radosNamespace":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":"","mirrorDaemonCount":0},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300 mapping:{"node":{"a":{"Name":"kubernetes-2","Hostname":"kubernetes-2","Address":"10.100.50.102"},"b":{"Name":"kubernetes-1","Hostname":"kubernetes-1","Address":"10.100.50.101"},"c":{"Name":"kubernetes-0","Hostname":"kubernetes-0","Address":"10.100.50.100"}}} maxMonId:2 outOfQuorum:]
2025-01-29 15:47:35.621210 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:47:35.621442 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:47:35.877511 I | op-mon: deployment for mon rook-ceph-mon-b already exists. updating if needed
2025-01-29 15:47:35.891807 I | op-k8sutil: deployment "rook-ceph-mon-b" did not change, nothing to update
2025-01-29 15:47:35.891841 I | op-mon: waiting for mon quorum with [b c a]
2025-01-29 15:47:36.465648 I | op-mon: mons running: [b c a]
2025-01-29 15:47:37.300636 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:47:37.309539 I | op-mon: deployment for mon rook-ceph-mon-c already exists. updating if needed
2025-01-29 15:47:37.320886 I | op-k8sutil: deployment "rook-ceph-mon-c" did not change, nothing to update
2025-01-29 15:47:37.320909 I | op-mon: waiting for mon quorum with [b c a]
2025-01-29 15:47:37.364109 I | op-mon: mons running: [b c a]
2025-01-29 15:47:37.984166 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:47:37.993247 I | op-mon: deployment for mon rook-ceph-mon-a already exists. updating if needed
2025-01-29 15:47:38.005502 I | op-k8sutil: deployment "rook-ceph-mon-a" did not change, nothing to update
2025-01-29 15:47:38.005533 I | op-mon: waiting for mon quorum with [b c a]
2025-01-29 15:47:38.049564 I | op-mon: mons running: [b c a]
2025-01-29 15:47:38.130920 I | clusterdisruption-controller: all "host" failure domains: []. osd is down in failure domain: "". active node drains: false. pg health: "cluster is not fully clean. PGs: [{StateName:unknown Count:1}]"
2025-01-29 15:47:38.136647 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2025-01-29 15:47:38.625725 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:47:38.625771 I | op-mon: mons created: 3
2025-01-29 15:47:39.172046 I | op-mon: waiting for mon quorum with [b c a]
2025-01-29 15:47:39.217776 I | op-mon: mons running: [b c a]
2025-01-29 15:47:39.385250 I | clusterdisruption-controller: all "host" failure domains: []. osd is down in failure domain: "". active node drains: false. pg health: "cluster is not fully clean. PGs: [{StateName:unknown Count:1}]"
2025-01-29 15:47:39.391209 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2025-01-29 15:47:39.752267 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:47:39.752309 I | ceph-spec: ensuring cluster "rook-ceph" "public" network is configured to use CIDR(s) ""
2025-01-29 15:47:39.752320 I | ceph-spec: ensuring cluster "rook-ceph" "cluster" network is configured to use CIDR(s) ""
2025-01-29 15:47:39.752609 I | cephclient: getting or creating ceph auth key "client.csi-rbd-provisioner"
2025-01-29 15:47:40.282876 I | cephclient: getting or creating ceph auth key "client.csi-rbd-node"
2025-01-29 15:47:40.819403 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-provisioner"
2025-01-29 15:47:41.397345 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-node"
2025-01-29 15:47:42.026685 I | ceph-csi: created kubernetes csi secrets for cluster "rook-ceph"
2025-01-29 15:47:42.026713 I | cephclient: getting or creating ceph auth key "client.crash"
2025-01-29 15:47:42.262668 I | exec: exec timeout waiting for process rbd to return. Sending interrupt signal to the process
2025-01-29 15:47:42.277112 E | ceph-block-pool-controller: failed to reconcile CephBlockPool "rook-ceph/ceph-blockpool". failed to create pool "ceph-blockpool".: failed to configure pool "ceph-blockpool".: failed to initialize pool "ceph-blockpool" for RBD use. : signal: interrupt
2025-01-29 15:47:42.640390 I | ceph-nodedaemon-controller: created kubernetes crash collector secret for cluster "rook-ceph"
2025-01-29 15:47:42.640432 I | cephclient: getting or creating ceph auth key "client.ceph-exporter"
2025-01-29 15:47:43.267430 I | ceph-nodedaemon-controller: created kubernetes exporter secret for cluster "rook-ceph"
2025-01-29 15:47:43.267463 I | op-config: deleting "global" "ms_cluster_mode" option from the mon configuration database
2025-01-29 15:47:43.701474 I | op-config: successfully deleted "ms_cluster_mode" option from the mon configuration database
2025-01-29 15:47:43.701510 I | op-config: deleting "global" "ms_service_mode" option from the mon configuration database
2025-01-29 15:47:44.103020 I | op-config: successfully deleted "ms_service_mode" option from the mon configuration database
2025-01-29 15:47:44.103057 I | op-config: deleting "global" "ms_client_mode" option from the mon configuration database
2025-01-29 15:47:44.527267 I | op-config: successfully deleted "ms_client_mode" option from the mon configuration database
2025-01-29 15:47:44.527308 I | op-config: deleting "global" "rbd_default_map_options" option from the mon configuration database
2025-01-29 15:47:45.232699 I | op-config: successfully deleted "rbd_default_map_options" option from the mon configuration database
2025-01-29 15:47:45.233093 I | op-config: applying ceph settings:
[global]
rbd_default_map_options = ms_mode=prefer-crc
2025-01-29 15:47:45.660072 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:47:45.660180 I | op-config: deleting "global" "ms_osd_compress_mode" option from the mon configuration database
2025-01-29 15:47:46.094931 I | op-config: successfully deleted "ms_osd_compress_mode" option from the mon configuration database
2025-01-29 15:47:46.094968 I | cephclient: create rbd-mirror bootstrap peer token "client.rbd-mirror-peer"
2025-01-29 15:47:46.094976 I | cephclient: getting or creating ceph auth key "client.rbd-mirror-peer"
2025-01-29 15:47:46.629122 I | cephclient: successfully created rbd-mirror bootstrap peer token for cluster "rook-ceph"
2025-01-29 15:47:46.669527 I | op-mgr: start running mgr
2025-01-29 15:47:46.680011 I | cephclient: getting or creating ceph auth key "mgr.a"
2025-01-29 15:47:47.229587 I | op-mgr: deployment for mgr rook-ceph-mgr-a already exists. updating if needed
2025-01-29 15:47:47.244106 I | op-k8sutil: deployment "rook-ceph-mgr-a" did not change, nothing to update
2025-01-29 15:47:47.244136 I | cephclient: getting or creating ceph auth key "mgr.b"
2025-01-29 15:47:47.824438 I | op-mgr: deployment for mgr rook-ceph-mgr-b already exists. updating if needed
2025-01-29 15:47:47.839220 I | op-k8sutil: deployment "rook-ceph-mgr-b" did not change, nothing to update
2025-01-29 15:47:47.940616 E | ceph-cluster-controller: failed to reconcile CephCluster "rook-ceph/rook-ceph". failed to reconcile cluster "rook-ceph": failed to configure local ceph cluster: failed to create cluster: failed to start ceph mgr: failed to enable mgr services: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-mgr" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:47:52.526759 I | ceph-spec: parsing mon endpoints: a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300
2025-01-29 15:47:53.268776 I | ceph-block-pool-controller: creating pool "ceph-blockpool" in namespace "rook-ceph"
2025-01-29 15:47:54.634724 I | cephclient: application "rbd" is already set on pool "ceph-blockpool"
2025-01-29 15:47:54.634756 I | cephclient: reconciling replicated pool ceph-blockpool succeeded
2025-01-29 15:47:54.634768 I | ceph-block-pool-controller: initializing pool "ceph-blockpool" for RBD use
2025-01-29 15:47:58.182013 I | ceph-cluster-controller: reconciling ceph cluster in namespace "rook-ceph"
2025-01-29 15:47:58.191464 I | ceph-spec: parsing mon endpoints: a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300
2025-01-29 15:47:58.237963 I | ceph-spec: detecting the ceph image version for image quay.io/ceph/ceph:v19.2.0...
2025-01-29 15:48:00.482637 I | ceph-spec: detected ceph image version: "19.2.0-0 squid"
2025-01-29 15:48:00.482665 I | ceph-cluster-controller: validating ceph version from provided image
2025-01-29 15:48:00.491338 I | ceph-spec: parsing mon endpoints: a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300
2025-01-29 15:48:00.496208 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:48:00.496490 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:48:01.223407 I | ceph-cluster-controller: cluster "rook-ceph": version "19.2.0-0 squid" detected for image "quay.io/ceph/ceph:v19.2.0"
2025-01-29 15:48:01.324421 I | op-mon: start running mons
2025-01-29 15:48:01.338141 I | ceph-spec: parsing mon endpoints: a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300
2025-01-29 15:48:01.381397 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.100.50.102:3300","10.100.50.101:3300","10.100.50.100:3300"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","radosNamespace":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":"","mirrorDaemonCount":0},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300 mapping:{"node":{"a":{"Name":"kubernetes-2","Hostname":"kubernetes-2","Address":"10.100.50.102"},"b":{"Name":"kubernetes-1","Hostname":"kubernetes-1","Address":"10.100.50.101"},"c":{"Name":"kubernetes-0","Hostname":"kubernetes-0","Address":"10.100.50.100"}}} maxMonId:2 outOfQuorum:]
2025-01-29 15:48:01.669211 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:48:01.669568 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:48:03.269294 I | op-mon: targeting the mon count 3
2025-01-29 15:48:03.290859 I | op-config: applying ceph settings:
[global]
mon allow pool delete   = true
mon cluster log file    = 
mon allow pool size one = true
2025-01-29 15:48:03.751578 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:48:03.751886 I | op-config: applying ceph settings:
[global]
log to file = true
2025-01-29 15:48:04.164436 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:48:04.164533 I | op-config: deleting "global" "log file" option from the mon configuration database
2025-01-29 15:48:04.632919 I | op-config: successfully deleted "log file" option from the mon configuration database
2025-01-29 15:48:04.632960 I | op-mon: checking for basic quorum with existing mons
2025-01-29 15:48:04.632973 I | op-mon: setting mon "a" endpoints for hostnetwork mode
2025-01-29 15:48:04.632987 I | op-mon: setting mon "b" endpoints for hostnetwork mode
2025-01-29 15:48:04.632995 I | op-mon: setting mon "c" endpoints for hostnetwork mode
2025-01-29 15:48:04.653575 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.100.50.102:3300","10.100.50.101:3300","10.100.50.100:3300"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","radosNamespace":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":"","mirrorDaemonCount":0},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300 mapping:{"node":{"a":{"Name":"kubernetes-2","Hostname":"kubernetes-2","Address":"10.100.50.102"},"b":{"Name":"kubernetes-1","Hostname":"kubernetes-1","Address":"10.100.50.101"},"c":{"Name":"kubernetes-0","Hostname":"kubernetes-0","Address":"10.100.50.100"}}} maxMonId:2 outOfQuorum:]
2025-01-29 15:48:04.668597 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:48:04.668905 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:48:04.891097 I | op-mon: deployment for mon rook-ceph-mon-a already exists. updating if needed
2025-01-29 15:48:04.906273 I | op-k8sutil: deployment "rook-ceph-mon-a" did not change, nothing to update
2025-01-29 15:48:04.906307 I | op-mon: waiting for mon quorum with [a b c]
2025-01-29 15:48:05.477676 I | op-mon: mons running: [a b c]
2025-01-29 15:48:06.292279 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:48:06.301509 I | op-mon: deployment for mon rook-ceph-mon-b already exists. updating if needed
2025-01-29 15:48:06.316259 I | op-k8sutil: deployment "rook-ceph-mon-b" did not change, nothing to update
2025-01-29 15:48:06.316288 I | op-mon: waiting for mon quorum with [a b c]
2025-01-29 15:48:06.368800 I | op-mon: mons running: [a b c]
2025-01-29 15:48:06.928062 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:48:06.935424 I | op-mon: deployment for mon rook-ceph-mon-c already exists. updating if needed
2025-01-29 15:48:06.945183 I | op-k8sutil: deployment "rook-ceph-mon-c" did not change, nothing to update
2025-01-29 15:48:06.945203 I | op-mon: waiting for mon quorum with [a b c]
2025-01-29 15:48:06.981596 I | op-mon: mons running: [a b c]
2025-01-29 15:48:07.768986 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:48:07.769022 I | op-mon: mons created: 3
2025-01-29 15:48:08.305553 I | op-mon: waiting for mon quorum with [a b c]
2025-01-29 15:48:08.350052 I | op-mon: mons running: [a b c]
2025-01-29 15:48:08.687224 I | clusterdisruption-controller: all "host" failure domains: []. osd is down in failure domain: "". active node drains: false. pg health: "cluster is not fully clean. PGs: [{StateName:unknown Count:1}]"
2025-01-29 15:48:08.693203 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2025-01-29 15:48:08.929918 I | op-mon: Monitors in quorum: [a b c]
2025-01-29 15:48:08.929956 I | ceph-spec: ensuring cluster "rook-ceph" "public" network is configured to use CIDR(s) ""
2025-01-29 15:48:08.929965 I | ceph-spec: ensuring cluster "rook-ceph" "cluster" network is configured to use CIDR(s) ""
2025-01-29 15:48:08.930241 I | cephclient: getting or creating ceph auth key "client.csi-rbd-provisioner"
2025-01-29 15:48:09.636190 I | exec: exec timeout waiting for process rbd to return. Sending interrupt signal to the process
2025-01-29 15:48:09.652024 E | ceph-block-pool-controller: failed to reconcile CephBlockPool "rook-ceph/ceph-blockpool". failed to create pool "ceph-blockpool".: failed to configure pool "ceph-blockpool".: failed to initialize pool "ceph-blockpool" for RBD use. : signal: interrupt
2025-01-29 15:48:09.733018 I | cephclient: getting or creating ceph auth key "client.csi-rbd-node"
2025-01-29 15:48:10.153197 I | clusterdisruption-controller: all "host" failure domains: []. osd is down in failure domain: "". active node drains: false. pg health: "cluster is not fully clean. PGs: [{StateName:unknown Count:1}]"
2025-01-29 15:48:10.162724 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2025-01-29 15:48:10.259021 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-provisioner"
2025-01-29 15:48:10.794426 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-node"
2025-01-29 15:48:11.348525 I | ceph-csi: created kubernetes csi secrets for cluster "rook-ceph"
2025-01-29 15:48:11.348555 I | cephclient: getting or creating ceph auth key "client.crash"
2025-01-29 15:48:11.964987 I | ceph-nodedaemon-controller: created kubernetes crash collector secret for cluster "rook-ceph"
2025-01-29 15:48:11.965012 I | cephclient: getting or creating ceph auth key "client.ceph-exporter"
2025-01-29 15:48:12.721639 I | ceph-nodedaemon-controller: created kubernetes exporter secret for cluster "rook-ceph"
2025-01-29 15:48:12.721674 I | op-config: deleting "global" "rbd_default_map_options" option from the mon configuration database
2025-01-29 15:48:13.143331 I | op-config: successfully deleted "rbd_default_map_options" option from the mon configuration database
2025-01-29 15:48:13.143362 I | op-config: deleting "global" "ms_cluster_mode" option from the mon configuration database
2025-01-29 15:48:13.585882 I | op-config: successfully deleted "ms_cluster_mode" option from the mon configuration database
2025-01-29 15:48:13.585913 I | op-config: deleting "global" "ms_service_mode" option from the mon configuration database
2025-01-29 15:48:14.047612 I | op-config: successfully deleted "ms_service_mode" option from the mon configuration database
2025-01-29 15:48:14.047644 I | op-config: deleting "global" "ms_client_mode" option from the mon configuration database
2025-01-29 15:48:14.550236 I | op-config: successfully deleted "ms_client_mode" option from the mon configuration database
2025-01-29 15:48:14.550616 I | op-config: applying ceph settings:
[global]
rbd_default_map_options = ms_mode=prefer-crc
2025-01-29 15:48:14.979397 I | op-config: successfully applied settings to the mon configuration database
2025-01-29 15:48:14.979512 I | op-config: deleting "global" "ms_osd_compress_mode" option from the mon configuration database
2025-01-29 15:48:15.583993 I | op-config: successfully deleted "ms_osd_compress_mode" option from the mon configuration database
2025-01-29 15:48:15.584037 I | cephclient: create rbd-mirror bootstrap peer token "client.rbd-mirror-peer"
2025-01-29 15:48:15.584047 I | cephclient: getting or creating ceph auth key "client.rbd-mirror-peer"
2025-01-29 15:48:16.073004 I | cephclient: successfully created rbd-mirror bootstrap peer token for cluster "rook-ceph"
2025-01-29 15:48:16.114946 I | op-mgr: start running mgr
2025-01-29 15:48:16.125656 I | cephclient: getting or creating ceph auth key "mgr.a"
2025-01-29 15:48:16.707620 I | op-mgr: deployment for mgr rook-ceph-mgr-a already exists. updating if needed
2025-01-29 15:48:16.722163 I | op-k8sutil: deployment "rook-ceph-mgr-a" did not change, nothing to update
2025-01-29 15:48:16.722192 I | cephclient: getting or creating ceph auth key "mgr.b"
2025-01-29 15:48:17.244632 I | op-mgr: deployment for mgr rook-ceph-mgr-b already exists. updating if needed
2025-01-29 15:48:17.259453 I | op-k8sutil: deployment "rook-ceph-mgr-b" did not change, nothing to update
2025-01-29 15:48:17.363770 E | ceph-cluster-controller: failed to reconcile CephCluster "rook-ceph/rook-ceph". failed to reconcile cluster "rook-ceph": failed to configure local ceph cluster: failed to create cluster: failed to start ceph mgr: failed to enable mgr services: failed to enable service monitor: service monitor could not be enabled: failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com "rook-ceph-mgr" is forbidden: User "system:serviceaccount:rook-ceph:rook-ceph-system" cannot get resource "servicemonitors" in API group "monitoring.coreos.com" in the namespace "rook-ceph"
2025-01-29 15:48:30.142897 I | ceph-spec: parsing mon endpoints: a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300
2025-01-29 15:48:30.611929 I | ceph-block-pool-controller: creating pool "ceph-blockpool" in namespace "rook-ceph"
2025-01-29 15:48:32.058180 I | cephclient: application "rbd" is already set on pool "ceph-blockpool"
2025-01-29 15:48:32.058211 I | cephclient: reconciling replicated pool ceph-blockpool succeeded
2025-01-29 15:48:32.058228 I | ceph-block-pool-controller: initializing pool "ceph-blockpool" for RBD use
2025-01-29 15:48:37.844716 I | ceph-cluster-controller: reconciling ceph cluster in namespace "rook-ceph"
2025-01-29 15:48:37.855764 I | ceph-spec: parsing mon endpoints: a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300
2025-01-29 15:48:37.914209 I | ceph-spec: detecting the ceph image version for image quay.io/ceph/ceph:v19.2.0...
2025-01-29 15:48:39.249247 I | clusterdisruption-controller: all "host" failure domains: []. osd is down in failure domain: "". active node drains: false. pg health: "cluster is not fully clean. PGs: [{StateName:unknown Count:1}]"
2025-01-29 15:48:39.255067 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2025-01-29 15:48:39.792624 I | ceph-spec: detected ceph image version: "19.2.0-0 squid"
2025-01-29 15:48:39.792655 I | ceph-cluster-controller: validating ceph version from provided image
2025-01-29 15:48:39.801936 I | ceph-spec: parsing mon endpoints: a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300
2025-01-29 15:48:39.806302 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:48:39.806516 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2025-01-29 15:48:40.353667 I | ceph-cluster-controller: cluster "rook-ceph": version "19.2.0-0 squid" detected for image "quay.io/ceph/ceph:v19.2.0"
2025-01-29 15:48:40.447648 I | op-mon: start running mons
2025-01-29 15:48:40.455841 I | ceph-spec: parsing mon endpoints: a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300
2025-01-29 15:48:40.484587 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.100.50.102:3300","10.100.50.101:3300","10.100.50.100:3300"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","radosNamespace":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":"","mirrorDaemonCount":0},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.100.50.102:3300,b=10.100.50.101:3300,c=10.100.50.100:3300 mapping:{"node":{"a":{"Name":"kubernetes-2","Hostname":"kubernetes-2","Address":"10.100.50.102"},"b":{"Name":"kubernetes-1","Hostname":"kubernetes-1","Address":"10.100.50.101"},"c":{"Name":"kubernetes-0","Hostname":"kubernetes-0","Address":"10.100.50.100"}}} maxMonId:2 outOfQuorum:]
2025-01-29 15:48:40.726427 I | clusterdisruption-controller: all "host" failure domains: []. osd is down in failure domain: "". active node drains: false. pg health: "cluster is not fully clean. PGs: [{StateName:unknown Count:1}]"
2025-01-29 15:48:40.734171 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2025-01-29 15:48:40.976226 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2025-01-29 15:48:40.976515 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
